{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EettNg4TW5f",
        "outputId": "38498037-12e2-4896-907f-ccb0ec53064f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAclJ5EAtxyL"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-EV7vrK_q9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWLl22bM6LSW"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "Bz4UqDhjt1ED",
        "outputId": "d6664b8c-5d73-40dc-e385-d128213aa1a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfangyua\u001b[0m (\u001b[33mfangyua-univeristy-of-michigan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WmNzSf047AQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBnXU-eyOwC4"
      },
      "outputs": [],
      "source": [
        "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '+', '&', '*']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "padding_token_index = 13\n",
        "end_token_index = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e4EbjUWPBjj",
        "outputId": "7ec06cae-c63a-498c-d3cd-8a353eee1010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 11, 2, 10, 3, 12]\n",
            "1+2=3&\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from chars to ints\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n",
        "\n",
        "print(encode(\"1+2=3&\"))\n",
        "print(decode(encode(\"1+2=3&\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRve1V8sonv6",
        "outputId": "e89d8702-1c8d-43c6-d7e0-29f8323c4af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPUNSWqZO2w"
      },
      "outputs": [],
      "source": [
        "def get_batch(phase=None, batch_size=1024, block_size=25, mode='train'):\n",
        "\n",
        "    if mode == 'train':\n",
        "      # random choose a and b from set\n",
        "      if phase != 6:\n",
        "          a = np.random.randint(0, 10**(phase), batch_size)\n",
        "          b = np.random.randint(0, 10**(phase), batch_size)\n",
        "          c = a + b\n",
        "      elif phase == 6:\n",
        "          exp_a = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          exp_b = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          # print(exp)\n",
        "          a = np.random.randint(10**(exp_a-1), 10**(exp_a), size=batch_size)\n",
        "          b = np.random.randint(10**(exp_b-1), 10**(exp_b), size=batch_size)\n",
        "          c = a + b\n",
        "    else:\n",
        "      if phase != 6:\n",
        "          a = np.random.randint(0, 10**(phase), batch_size)\n",
        "          b = np.random.randint(0, 10**(phase), batch_size)\n",
        "          c = a + b\n",
        "      elif phase == 6:\n",
        "          exp_a = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          exp_b = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          # print(exp)\n",
        "          a = np.random.randint(10**(exp_a-1), 10**(exp_a), size=batch_size)\n",
        "          b = np.random.randint(10**(exp_b-1), 10**(exp_b), size=batch_size)\n",
        "          c = a + b\n",
        "\n",
        "    x_list, y_list = [], []\n",
        "    for i, j, k in zip(a, b, c):\n",
        "        # construct X: \"i+j=k&\"\n",
        "        i_str = str(i)[::-1]\n",
        "        j_str = str(j)[::-1]\n",
        "        k_str = str(k)[::-1]\n",
        "        x_str = f\"{i_str}+{j_str}={k_str}&\"\n",
        "        # print(x_str)\n",
        "        x_encoded = encode(x_str)\n",
        "        x_padded = x_encoded + [padding_token_index] * (block_size - len(x_encoded))\n",
        "        x_list.append(torch.tensor(x_padded, dtype=torch.int64))\n",
        "\n",
        "        # construct Y: \"k&\"\n",
        "        y_encoded = encode(x_str)[1:]\n",
        "        y_encoded.append(end_token_index)\n",
        "        y_padded = y_encoded + [padding_token_index] * (block_size - len(y_encoded))\n",
        "        y_list.append(torch.tensor(y_padded, dtype=torch.int64))\n",
        "\n",
        "    x_tensor = torch.stack(x_list).to(device)\n",
        "    y_tensor = torch.stack(y_list).to(device)\n",
        "    return x_tensor, y_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fbr7rBey6SD",
        "outputId": "18a30baa-5eef-4757-e099-af9f54692f61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1024, 25])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_batch(phase=6)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVIg1aizaT7A"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias=True): # class constructor\n",
        "        super().__init__()\n",
        "        # nn.Parameter, pytorch optimize will update the value of this parameter during training\n",
        "        self.weight = nn.Parameter(torch.ones(ndim)) # trainable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # trainable parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Key, Query, Value projections\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
        "\n",
        "        # T-5 PE\n",
        "        # self.rel_pos_bias = T5RelativePositionBias(block_size, n_head)\n",
        "\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "                # Check for Flash Attention availability\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # Causal mask for slow attention\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimension\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # Split into Q, K, V (B, T, n_embd)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "\n",
        "        # Compute T5 relative position bias\n",
        "        # self.rel_pos_bias = self.rel_pos_bias.to(device)  # Move to correct device\n",
        "        # rel_bias = self.rel_pos_bias(T, device)  # Compute relative position bias\n",
        "        # (1, num_heads, T, T)\n",
        "\n",
        "        # Flash Attention or fallback to manual implementation\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True\n",
        "            )\n",
        "        # else:\n",
        "        # Manual attention with causal masking\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # Scaled dot product\n",
        "        # # att = att + rel_bias  # Apply relative positional bias\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))  # Apply causal mask\n",
        "        # att = F.softmax(att, dim=-1)  # Normalize attention scores\n",
        "        # att = self.attn_dropout(att)\n",
        "        # y = att @ v  # Apply attention weights to values (B, n_head, T, head_size)\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Reassemble heads\n",
        "\n",
        "        # Output projection and residual dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module): # FFN\n",
        "\n",
        "    def __init__(self, n_embd, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n",
        "        self.gelu    = nn.GELU() # nonlinear activation function\n",
        "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        # LayerNorm and CausalSelfAttention with explicit parameters\n",
        "        self.ln_1 = LayerNorm(n_embd, bias=bias)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=bias)\n",
        "        self.ln_2 = LayerNorm(n_embd, bias=bias)\n",
        "        self.mlp = MLP(n_embd, dropout, bias=bias)  # MLP with explicit parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply residual connection and pre-normalization\n",
        "        x = x + self.attn(self.ln_1(x))  # Apply LayerNorm before attention\n",
        "        x = x + self.mlp(self.ln_2(x))  # Apply LayerNorm before MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        super().__init__()\n",
        "        assert vocab_size is not None\n",
        "        assert block_size is not None\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n",
        "            # wpe = nn.Embedding(block_size, n_embd), # positional embeddings CHANGE, t-5 positional embedding\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n",
        "            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.cblock_size}\"\n",
        "        # pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb)# + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=13)\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            # loss = None\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNC6DqTgcbZl"
      },
      "outputs": [],
      "source": [
        "eval_iters = 100\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(phase, models):\n",
        "    out = {}\n",
        "    models.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(phase, mode='train')\n",
        "        padding_mask_x = (X != padding_token_index).long()\n",
        "        logits, loss = models(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out['train'] = losses.mean()\n",
        "    models.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEKyA9IlOe5l"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=1, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate a sequence of tokens given an initial sequence.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model used for generation.\n",
        "        idx (torch.Tensor or list): Initial sequence of indices (LongTensor of shape (b,t)).\n",
        "        max_new_tokens (int): Number of new tokens to generate.\n",
        "        temperature (float): Scaling factor for logits before softmax.\n",
        "        top_k (int, optional): If specified, restricts sampling to top k tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence.\n",
        "    \"\"\"\n",
        "    idx = idx.unsqueeze(0) if idx.dim() == 1 else idx\n",
        "    idx = torch.tensor(idx, device=model.device) if not isinstance(idx, torch.Tensor) else idx.to(model.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Ensure context length does not exceed model's block size\n",
        "        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits, _ = model(idx_cond)\n",
        "\n",
        "        # Extract logits for the last token and apply temperature scaling\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply top-k filtering if necessary\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample next token\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == end_token_index:\n",
        "            break\n",
        "        # Append sampled token to sequence\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return decode(idx.tolist()[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOdwJ26Ax2e6"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024 # how many independent sequences will we process in parallel?\n",
        "block_size = 25 # what is the maximum context length for predictions?\n",
        "max_iters = 20000 # CHANGE the step size\n",
        "eval_interval = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "bias = True \n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "JG9utTPZvaU-",
        "outputId": "cbfb3295-b244-4a36-dae2-f8abf1bd5bf4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250301_234035-7z339az0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0' target=\"_blank\">Trial 6 of finding 90%+ accuracy</a></strong> to <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7abf61f17c50>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"self-improve-transformer-train\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 25,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            },\n",
        "           name = \"Trial 6 of finding 90%+ accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQpX_YZlYluM"
      },
      "outputs": [],
      "source": [
        "wandb.run.notes = \"Trial 5 of finding \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPl7iF7e2D-n"
      },
      "outputs": [],
      "source": [
        "# run this cell if you wanna initialize a new model\n",
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snZoFt8qe76q"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer for base model\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,                  # 学习率 change to 1e-4 from 5e-4\n",
        "    betas=(0.9, 0.99),        # 动量参数\n",
        "    eps=1e-12,                # 数值稳定性参数\n",
        "    weight_decay=0.1          # 权重衰减\n",
        ")\n",
        "\n",
        "total_steps = 20000       # 总训练步数 CHANGE\n",
        "warmup_steps = 1000       # 预热阶段步数\n",
        "decay_steps = 2000        # 衰减阶段步数\n",
        "stable_steps = total_steps - warmup_steps - decay_steps  # 稳定阶段步数\n",
        "\n",
        "# Warmup + Stable + Decay\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        # linear Warmup（0 to base_lr）\n",
        "        return step / warmup_steps\n",
        "    elif step < warmup_steps + stable_steps:\n",
        "        # stable\n",
        "        return 1.0\n",
        "    else:\n",
        "        # Cosine Decay）\n",
        "        decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "        return 0.5 * (1 + math.cos(math.pi * decay_ratio))  # cos 1 to 0\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odtqfBE0swtz",
        "outputId": "d72d51e0-3bfb-4789-f0b3-3c699943ce23"
      },
      "outputs": [],
      "source": [
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "phase = 6\n",
        "counter = 0\n",
        "best_loss = float('inf')\n",
        "val_loss_list = []\n",
        "\n",
        "patience = 20\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "for iter in tqdm(range(max_iters), desc=\"Training Progress\"):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(phase, model)['train']\n",
        "        print(f\"step {iter}: loss {losses:.4f}, Best Loss so far: {best_loss}, Counter: {counter}\")\n",
        "        log_dict = {\"Loss\": losses}\n",
        "        val_loss_list.append(round(losses.item(), 4))\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(phase)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBC5BDrbf49W"
      },
      "outputs": [],
      "source": [
        "# Helper function for accuracy printing for each model\n",
        "def accuracy_print_one(model, num_digits, need_print=False):\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "\n",
        "    for j in range(100):  # CHANGE the num_samples = 100\n",
        "        a = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "        b = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "        c = a + b\n",
        "        reversed_a = np.array([str(x)[::-1] for x in a])\n",
        "        reversed_b = np.array([str(x)[::-1] for x in b])\n",
        "        reversed_c = np.array([str(x)[::-1] for x in c])\n",
        "        input = f\"{reversed_a.item()}+{reversed_b.item()}=\"\n",
        "        context = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
        "\n",
        "        output1 = generate(model=model, idx=context, max_new_tokens=35, top_k=1)\n",
        "\n",
        "        if output1 == f\"{reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\":\n",
        "            correct += 1\n",
        "            wrong = 0\n",
        "        else:\n",
        "            wrong = 1\n",
        "\n",
        "        # for testing\n",
        "        if need_print and wrong:\n",
        "            print(f\"   Input: {input}\")\n",
        "            print(f\"  Output: {output1}\")\n",
        "            print(f\"Expected: {reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\")\n",
        "            print(\"-----------\")\n",
        "\n",
        "    acc = correct / 100\n",
        "    print(f\"Accuracy for {num_digits} digits addition: {acc} \")\n",
        "    return acc\n",
        "    \n",
        "def get_avg_performance(model):\n",
        "    '''\n",
        "    Call this function for get the accuracy for each model\n",
        "    '''\n",
        "    dict_acc = {}\n",
        "    for num_dig in range(1, 8):\n",
        "        dict_acc[num_dig] = accuracy_print_one(model, num_dig, need_print=False)\n",
        "    return dict_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj9N9BGSQDoX"
      },
      "outputs": [],
      "source": [
        "# Helper function for multiple training models for 90%+ accuracy\n",
        "def test_accuracy_on_6(model):\n",
        "    return accuracy_print_one(model, 6, need_print=False)\n",
        "\n",
        "def create_optimizer_and_scheduler(model):\n",
        "    # AdamW\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,              # learning rate\n",
        "        betas=(0.9, 0.99),\n",
        "        eps=1e-12,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # LR Scheduler\n",
        "    total_steps = 20000 # CHANGE, CHECK max_iter\n",
        "    warmup_steps = 1000\n",
        "    decay_steps = 2000\n",
        "    stable_steps = total_steps - warmup_steps - decay_steps\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps  # Linear warmup 0->1\n",
        "        elif step < warmup_steps + stable_steps:\n",
        "            return 1.0                  # Stable\n",
        "        else:\n",
        "            # Cosine decay from 1->0\n",
        "            decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "            return 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "q62fWUVdQHgV",
        "outputId": "733985d5-4b07-45b5-bc69-8ad5df31ef33"
      },
      "outputs": [],
      "source": [
        "# train the model multiple times and save the model if accuracy on 6-digits addition is above this threshold\n",
        "\n",
        "num_runs = 5\n",
        "threshold = 0.85 \n",
        "\n",
        "for run_idx in range(num_runs):\n",
        "    print(f\"Start run {run_idx + 1}/{num_runs}\")\n",
        "\n",
        "    # INITIALIZE MODEL, OPTIMIZER, SHCEDULER\n",
        "    \n",
        "    # each time, initialize a new model with different random seed\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "    m = model.to(device)\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model)\n",
        "\n",
        "    # TRAINNG LOOP:\n",
        "    # print the number of parameters in the model\n",
        "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "    phase = 6\n",
        "    counter = 0\n",
        "    best_loss = float('inf')\n",
        "    val_loss_list = []\n",
        "\n",
        "    patience = 20\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "    for iter in tqdm(range(max_iters), desc=\"Training Progress\"):\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses1 = estimate_loss(phase, model)['train']\n",
        "            print(f\"step {iter}: loss {losses1:.4f}, Best Loss so far: {best_loss}, Counter: {counter}\")\n",
        "            log_dict = {\"Loss\": losses1}\n",
        "            val_loss_list.append(round(losses1.item(), 4))\n",
        "\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch(phase)\n",
        "\n",
        "        # evaluate the loss\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "            logits1, loss1 = model(xb, yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        scaler.scale(loss1).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "    print(f\"Training finished for run {run_idx+1}.\\nEvaluating 6-digit accuracy...\")\n",
        "\n",
        "    # evaluate final performance on 6-digit addition\n",
        "    acc_on_6 = test_accuracy_on_6(model)\n",
        "\n",
        "    # if the accuracy is above the threshold, save the model on google drive\n",
        "    if acc_on_6 > threshold:\n",
        "        filename = f\"run_{run_idx}_acc_{acc_on_6:.2f}.pt\"\n",
        "        save_path = f\"/content/drive/MyDrive/URPS/{filename}\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Saved best model at {save_path}\")\n",
        "\n",
        "print(\"All runs complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3aMpDXFrllU",
        "outputId": "bdef2c5a-4b19-447c-c70d-c73c2a0b2e77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-129-ff04680666f8>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model1.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.9 \n",
            "Accuracy on 6-digit addition of base model: 0.9\n"
          ]
        }
      ],
      "source": [
        "model1 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model1.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/accuracy_0.96_3w_iter.pt\"\n",
        "model1.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model1.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wafRWVgrzNK",
        "outputId": "37e8eb8c-70c7-44e3-d009-6c88e07775e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-132-6fce3ffa9d5f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model2.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.91 \n"
          ]
        }
      ],
      "source": [
        "model2 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model2.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/model.pth\"\n",
        "model2.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model2.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJk1keP7r-Oe",
        "outputId": "61d12564-d091-4971-f115-6bfdda1c180c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-131-10d7336150d7>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model3.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.88 \n"
          ]
        }
      ],
      "source": [
        "model3 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model3.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/run_0_acc_0.95.pt\"\n",
        "model3.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model3.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6flQf2vsjmM"
      },
      "outputs": [],
      "source": [
        "models = [model1, model2, model3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj3Kp1l8ufFJ"
      },
      "outputs": [],
      "source": [
        "# Helper functions for generating self-improve data\n",
        "def generate_prompt_OOD(phase):\n",
        "    \"\"\"\n",
        "    Return a list of 'num_prompts' strings for reversed addition\n",
        "    with 'phase' digits each.\n",
        "    \"\"\"\n",
        "    a = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    b = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    prompt_str = f\"{str(a)[::-1]}+{str(b)[::-1]}=\"  # e.g. '123545+543261=' => phase = 6\n",
        "\n",
        "    return prompt_str\n",
        "\n",
        "def length_filter(predictions, threshold=threshold): # we set threshold = 1\n",
        "    \"\"\"\n",
        "    Filter out predictions that are shorter than max_len - threshold.\n",
        "    \"\"\"\n",
        "    max_len = max(len(pred) for pred in predictions) # 7\n",
        "    min_len = max_len - threshold # 6\n",
        "    filtered = [(pred) for pred in predictions if len(pred) >= min_len]\n",
        "    \n",
        "    # print(f\"Filtered out {len(predictions) - len(filtered)} short outputs; kept {len(filtered)}.\")\n",
        "    return filtered\n",
        "\n",
        "def pad_sequence(seq_tensor, block_size, padding_idx):\n",
        "        \"\"\"\n",
        "        Pad (or truncate) a single seq_tensor to length of block_size.\n",
        "        \"\"\"\n",
        "        seq_list = seq_tensor.tolist()\n",
        "        if len(seq_list) < block_size:\n",
        "            seq_list += [padding_idx] * (block_size - len(seq_list))\n",
        "        else:\n",
        "            seq_list = seq_list[:block_size] # make sure the length is block_size\n",
        "        return torch.tensor(seq_list, dtype=torch.long)\n",
        "\n",
        "def majority_vote_filter(pred_tensors, vote_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Find the most frequent prediction of a given prompt.\n",
        "    If the most frequent prediction appears more than 'vote_threshold' times,\n",
        "    return the prediction. Otherwise, return None.\n",
        "    \"\"\"\n",
        "    if not pred_tensors:\n",
        "        return None\n",
        "\n",
        "    num_models = len(pred_tensors)\n",
        "    needed_votes = math.ceil(vote_threshold * num_models) # ceil(5 * 0.6) = 3, ceil(3 * 0.6) = 2\n",
        "\n",
        "    # decode each tensor into string\n",
        "    string_list = []\n",
        "    for t in pred_tensors:\n",
        "        string_list.append(decode(t.tolist())) # string_list contains all the predicted strings\n",
        "\n",
        "    freq = {}\n",
        "    for s in string_list:\n",
        "        freq[s] = freq.get(s, 0) + 1\n",
        "\n",
        "    # find most frequent string\n",
        "    # best_str is the most frequent string\n",
        "    # best_count is how many times it appears\n",
        "    best_str, best_count = None, 0\n",
        "    for text, count in freq.items():\n",
        "        if count > best_count:\n",
        "            best_str = text\n",
        "            best_count = count\n",
        "\n",
        "    # compare the frequency with the threshold\n",
        "    if best_count >= needed_votes:\n",
        "        best_ids = encode(best_str)\n",
        "        return torch.tensor(best_ids, dtype=torch.long)\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB7L1OixsSu5"
      },
      "outputs": [],
      "source": [
        "def generate_multi_model_length_vote_with_counter(\n",
        "    models,                # list of models [M1, M2, ...]\n",
        "    num_samples=100,\n",
        "    batch_size=1024,\n",
        "    phase=6,                 # digit length for reversed addition\n",
        "    block_size=25,\n",
        "    padding_idx=padding_token_index,\n",
        "    max_new_tokens=50,\n",
        "    threshold=1,         # length-filter threshold\n",
        "    vote_threshold=0.6,    # 5 model 选 3 model\n",
        "    eos_idx=end_token_index,\n",
        "    device=device\n",
        "):\n",
        "\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    num_models = len(models)\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "    kept_count = 0\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        # generate 'batch_size' prompts of digit length 6\n",
        "        prompts = [generate_prompt_OOD(phase) for _ in range(batch_size)]\n",
        "\n",
        "        for prompt_str in prompts: # iterate through all 1024 prompts\n",
        "\n",
        "            # encode and convert prompt_str into tensor\n",
        "            prompt_ids = encode(prompt_str)\n",
        "            prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long, device=device)\n",
        "\n",
        "            # collect each model's output for this prompt\n",
        "            model_outputs = []\n",
        "\n",
        "            for model_s in models:\n",
        "                out_str = generate(\n",
        "                    model=model_s,\n",
        "                    idx=prompt_tensor,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    top_k=1\n",
        "                )\n",
        "                # re-encode the generated string\n",
        "                out_ids = encode(out_str)\n",
        "                if eos_idx is not None:\n",
        "                    out_ids.append(eos_idx)\n",
        "\n",
        "                out_tensor = torch.tensor(out_ids, dtype=torch.long, device=device)\n",
        "                model_outputs.append(out_tensor) # len(model_outputs) == number of models\n",
        "\n",
        "            # length filter\n",
        "            # remove any model outputs that length < 6\n",
        "            filtered = length_filter(model_outputs, threshold=1)\n",
        "            if not filtered:\n",
        "                continue\n",
        "\n",
        "            # majority vote\n",
        "            best_tensor = majority_vote_filter(filtered, vote_threshold=0.6)\n",
        "            if best_tensor is None:\n",
        "                continue\n",
        "\n",
        "            # pad/truncate\n",
        "            seq_padded = pad_sequence(best_tensor, block_size, padding_idx)\n",
        "\n",
        "            # build (x, y)\n",
        "            xb = seq_padded[:-1]\n",
        "            yb = seq_padded[1:]\n",
        "            all_x.append(xb)\n",
        "            all_y.append(yb)\n",
        "            kept_count += 1\n",
        "\n",
        "    final_x = torch.stack(all_x, dim=0).to(device)\n",
        "    final_y = torch.stack(all_y, dim=0).to(device)\n",
        "\n",
        "    print(f\"Total final pairs kept: {kept_count}\")\n",
        "    return final_x, final_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UxRfkUErst5Z",
        "outputId": "96449990-89c0-4666-8fbf-0858183656af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total final pairs kept: 49314\n"
          ]
        }
      ],
      "source": [
        "x, y = generate_multi_model_length_vote_with_counter(models, num_samples=50000)\n",
        "self_improvement_data_6_digits = {\n",
        "    \"xb\": x,\n",
        "    \"yb\": y\n",
        "}\n",
        "save_path = \"/content/drive/MyDrive/URPS/Data/self_improvement_data_6_digits.pt\"\n",
        "torch.save(self_improvement_data_6_digits, save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = torch.load(\"/content/drive/MyDrive/URPS/Data/self_improvement_data_6_digits.pt\")\n",
        "self_improve_x = data[\"xb\"]\n",
        "self_improve_y = data[\"yb\"]\n",
        "print(self_improve_x)\n",
        "print(self_improve_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the optimizer and scheduler for self-improve round\n",
        "# model1 should be the pretrained model with highest accuracy\n",
        "optimizer1 = torch.optim.AdamW(\n",
        "    model1.parameters(),\n",
        "    lr=5e-4,                  # 学习率\n",
        "    betas=(0.9, 0.99),        # 动量参数\n",
        "    eps=1e-12,                # 数值稳定性参数\n",
        "    weight_decay=0.1          # 权重衰减\n",
        ")\n",
        "\n",
        "\n",
        "total_steps = 1500       # 总训练步数\n",
        "warmup_steps = 0       # 预热阶段步数\n",
        "decay_steps = 1500        # 衰减阶段步数\n",
        "stable_steps = total_steps - warmup_steps - decay_steps  # 稳定阶段步数\n",
        "\n",
        "# Warmup + Stable + Decay\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        # linear Warmup（0 to base_lr）\n",
        "        return step / warmup_steps\n",
        "    elif step < warmup_steps + stable_steps:\n",
        "        # stable\n",
        "        return 1.0\n",
        "    else:\n",
        "        # Cosine Decay）\n",
        "        decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "        return 0.5 * (1 + math.cos(math.pi * decay_ratio))  # cos 1 to 0\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fine-tuning the pretrained model\n",
        "print(sum(p.numel() for p in model1.parameters())/1e6, 'M parameters')\n",
        "\n",
        "phase = 6\n",
        "counter = 0\n",
        "best_loss = float('inf')\n",
        "val_loss_list = []\n",
        "\n",
        "patience = 20\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "for iter in tqdm(range(1500), desc=\"Training Progress\"):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses1 = estimate_loss(phase, model1)['train']\n",
        "        print(f\"step {iter}: loss {losses1:.4f}, Best Loss so far: {best_loss}, Counter: {counter}\")\n",
        "        log_dict = {\"Loss\": losses1}\n",
        "        val_loss_list.append(round(losses1.item(), 4))\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    if iter % 2 == 0:\n",
        "        # 50% iterations: use self-improvement data\n",
        "        # Randomly sample a batch from the self-improve dataset\n",
        "        indices = np.random.randint(0, self_improve_x.size(0), size=batch_size)\n",
        "        xb = self_improve_x[indices]\n",
        "        yb = self_improve_y[indices]\n",
        "        training_source = \"Self-improve data\"\n",
        "    else:\n",
        "        # 50% iterations: use online sampling (1-5 digit online data)\n",
        "        xb, yb = get_batch(phase=phase, batch_size=batch_size, block_size=block_size, mode='train')\n",
        "        training_source = \"Mixed digit training on Online Sampling (1-5 digits)\"\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits1, loss1 = model1(xb, yb)\n",
        "\n",
        "    optimizer1.zero_grad(set_to_none=True)\n",
        "\n",
        "    scaler.scale(loss1).backward()\n",
        "    scaler.step(optimizer1)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_performance = get_avg_performance(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRB1Uj3cU9Aw"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "os.system('git config --global user.email \"fangyua@umich.edu\"')\n",
        "os.system('git config --global user.name \"fangyua\"')\n",
        "\n",
        "# 2️⃣ **Use Google Drive to store GitHub Token**\n",
        "GITHUB_TOKEN_PATH = \"/content/drive/MyDrive/URPS/github_token.txt\"\n",
        "if os.path.exists(GITHUB_TOKEN_PATH):\n",
        "    with open(GITHUB_TOKEN_PATH, \"r\") as f:\n",
        "        os.environ[\"GITHUB_TOKEN\"] = f.read().strip()\n",
        "else:\n",
        "    print(\"❌ GitHub Token\")\n",
        "    exit(1)\n",
        "\n",
        "# 3️⃣ **Set up GitHub remote repo**\n",
        "GIT_PATH = \"/content/drive/MyDrive/URPS/Git\"\n",
        "REPO_URL = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/fangyua/self-improve-transformer.git\"\n",
        "\n",
        "if not os.path.exists(GIT_PATH):\n",
        "    print(f\"📁 Creating directory: {GIT_PATH}\")\n",
        "    os.makedirs(GIT_PATH)\n",
        "\n",
        "# 4️⃣ **If .git/ does not exsit， need to clone**\n",
        "if not os.path.exists(os.path.join(GIT_PATH, \".git\")):\n",
        "    print(\"❌ Git repository not found. Cloning...\")\n",
        "    subprocess.run(f\"rm -rf {GIT_PATH}\", shell=True, check=True)\n",
        "    subprocess.run(f\"git clone {REPO_URL} {GIT_PATH}\", shell=True, check=True)\n",
        "\n",
        "# 5️⃣ **Enter Git repo**\n",
        "os.chdir(GIT_PATH)\n",
        "print(\"📂 Changed working directory to:\", os.getcwd())\n",
        "\n",
        "\n",
        "# 6️⃣ **Check Git status**\n",
        "status_output = subprocess.run(\"git status\", shell=True, capture_output=True, text=True)\n",
        "print(status_output.stdout)\n",
        "\n",
        "#  **Push to Git**\n",
        "print(\"🚀 Adding files to Git...\")\n",
        "subprocess.run(\"git add .\", shell=True, check=True)\n",
        "\n",
        "print(\"📝 Committing changes...\")\n",
        "commit_output = subprocess.run('git commit -m \"Auto update from Google Colab 2.6\"', shell=True, capture_output=True, text=True)\n",
        "print(commit_output.stdout)\n",
        "\n",
        "\n",
        "\n",
        "print(\"📤 Pushing to GitHub...\")\n",
        "push_output = subprocess.run(\"git push origin main\", shell=True, capture_output=True, text=True)\n",
        "if \"fatal\" in push_output.stderr or \"error:\" in push_output.stderr:\n",
        "    print(\"❌ Real Git Push Error:\", push_output.stderr)\n",
        "else:\n",
        "    print(\"✅ Git Push Success!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
