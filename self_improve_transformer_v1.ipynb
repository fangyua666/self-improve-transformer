{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EettNg4TW5f",
        "outputId": "38498037-12e2-4896-907f-ccb0ec53064f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAclJ5EAtxyL"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-EV7vrK_q9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWLl22bM6LSW"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "Bz4UqDhjt1ED",
        "outputId": "d6664b8c-5d73-40dc-e385-d128213aa1a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfangyua\u001b[0m (\u001b[33mfangyua-univeristy-of-michigan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WmNzSf047AQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBnXU-eyOwC4"
      },
      "outputs": [],
      "source": [
        "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '+', '&', '*']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "padding_token_index = 13\n",
        "end_token_index = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e4EbjUWPBjj",
        "outputId": "7ec06cae-c63a-498c-d3cd-8a353eee1010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 11, 2, 10, 3, 12]\n",
            "1+2=3&\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from chars to ints\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n",
        "\n",
        "print(encode(\"1+2=3&\"))\n",
        "print(decode(encode(\"1+2=3&\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRve1V8sonv6",
        "outputId": "e89d8702-1c8d-43c6-d7e0-29f8323c4af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPUNSWqZO2w"
      },
      "outputs": [],
      "source": [
        "def get_batch(phase=None, batch_size=1024, block_size=25, mode='train'):\n",
        "\n",
        "    if mode == 'train':\n",
        "      # random choose a and b from set\n",
        "      if phase != 6:\n",
        "          a = np.random.randint(0, 10**(phase), batch_size)\n",
        "          b = np.random.randint(0, 10**(phase), batch_size)\n",
        "          c = a + b\n",
        "      elif phase == 6:\n",
        "          exp_a = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          exp_b = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          # print(exp)\n",
        "          a = np.random.randint(10**(exp_a-1), 10**(exp_a), size=batch_size)\n",
        "          b = np.random.randint(10**(exp_b-1), 10**(exp_b), size=batch_size)\n",
        "          c = a + b\n",
        "    else:\n",
        "      if phase != 6:\n",
        "          a = np.random.randint(0, 10**(phase), batch_size)\n",
        "          b = np.random.randint(0, 10**(phase), batch_size)\n",
        "          c = a + b\n",
        "      elif phase == 6:\n",
        "          exp_a = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          exp_b = np.random.choice(np.arange(1, 6), size=batch_size)\n",
        "          # print(exp)\n",
        "          a = np.random.randint(10**(exp_a-1), 10**(exp_a), size=batch_size)\n",
        "          b = np.random.randint(10**(exp_b-1), 10**(exp_b), size=batch_size)\n",
        "          c = a + b\n",
        "\n",
        "    x_list, y_list = [], []\n",
        "    for i, j, k in zip(a, b, c):\n",
        "        # construct X: \"i+j=k&\"\n",
        "        i_str = str(i)[::-1]\n",
        "        j_str = str(j)[::-1]\n",
        "        k_str = str(k)[::-1]\n",
        "        x_str = f\"{i_str}+{j_str}={k_str}&\"\n",
        "        # print(x_str)\n",
        "        x_encoded = encode(x_str)\n",
        "        x_padded = x_encoded + [padding_token_index] * (block_size - len(x_encoded))\n",
        "        x_list.append(torch.tensor(x_padded, dtype=torch.int64))\n",
        "\n",
        "        # construct Y: \"k&\"\n",
        "        y_encoded = encode(x_str)[1:]\n",
        "        y_encoded.append(end_token_index)\n",
        "        y_padded = y_encoded + [padding_token_index] * (block_size - len(y_encoded))\n",
        "        y_list.append(torch.tensor(y_padded, dtype=torch.int64))\n",
        "\n",
        "    x_tensor = torch.stack(x_list).to(device)\n",
        "    y_tensor = torch.stack(y_list).to(device)\n",
        "    return x_tensor, y_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fbr7rBey6SD",
        "outputId": "18a30baa-5eef-4757-e099-af9f54692f61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1024, 25])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_batch(phase=6)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVIg1aizaT7A"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias=True): # class constructor\n",
        "        super().__init__()\n",
        "        # nn.Parameter, pytorch optimize will update the value of this parameter during training\n",
        "        self.weight = nn.Parameter(torch.ones(ndim)) # trainable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # trainable parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Key, Query, Value projections\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
        "\n",
        "        # T-5 PE\n",
        "        # self.rel_pos_bias = T5RelativePositionBias(block_size, n_head)\n",
        "\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "                # Check for Flash Attention availability\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # Causal mask for slow attention\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimension\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # Split into Q, K, V (B, T, n_embd)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "\n",
        "        # Compute T5 relative position bias\n",
        "        # self.rel_pos_bias = self.rel_pos_bias.to(device)  # Move to correct device\n",
        "        # rel_bias = self.rel_pos_bias(T, device)  # Compute relative position bias\n",
        "        # (1, num_heads, T, T)\n",
        "\n",
        "        # Flash Attention or fallback to manual implementation\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True\n",
        "            )\n",
        "        # else:\n",
        "        # Manual attention with causal masking\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # Scaled dot product\n",
        "        # # att = att + rel_bias  # Apply relative positional bias\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))  # Apply causal mask\n",
        "        # att = F.softmax(att, dim=-1)  # Normalize attention scores\n",
        "        # att = self.attn_dropout(att)\n",
        "        # y = att @ v  # Apply attention weights to values (B, n_head, T, head_size)\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Reassemble heads\n",
        "\n",
        "        # Output projection and residual dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module): # FFN\n",
        "\n",
        "    def __init__(self, n_embd, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(n_embd, 4 * n_embd, bias=bias)\n",
        "        self.gelu    = nn.GELU() # nonlinear activation function\n",
        "        self.c_proj  = nn.Linear(4 * n_embd, n_embd, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        # LayerNorm and CausalSelfAttention with explicit parameters\n",
        "        self.ln_1 = LayerNorm(n_embd, bias=bias)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=bias)\n",
        "        self.ln_2 = LayerNorm(n_embd, bias=bias)\n",
        "        self.mlp = MLP(n_embd, dropout, bias=bias)  # MLP with explicit parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply residual connection and pre-normalization\n",
        "        x = x + self.attn(self.ln_1(x))  # Apply LayerNorm before attention\n",
        "        x = x + self.mlp(self.ln_2(x))  # Apply LayerNorm before MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        super().__init__()\n",
        "        assert vocab_size is not None\n",
        "        assert block_size is not None\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n",
        "            # wpe = nn.Embedding(block_size, n_embd), # positional embeddings CHANGE, t-5 positional embedding\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n",
        "            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.cblock_size}\"\n",
        "        # pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb)# + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=13)\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            # loss = None\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNC6DqTgcbZl"
      },
      "outputs": [],
      "source": [
        "eval_iters = 100\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(phase, models):\n",
        "    out = {}\n",
        "    models.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(phase, mode='train')\n",
        "        padding_mask_x = (X != padding_token_index).long()\n",
        "        logits, loss = models(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out['train'] = losses.mean()\n",
        "    models.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEKyA9IlOe5l"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=1, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate a sequence of tokens given an initial sequence.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model used for generation.\n",
        "        idx (torch.Tensor or list): Initial sequence of indices (LongTensor of shape (b,t)).\n",
        "        max_new_tokens (int): Number of new tokens to generate.\n",
        "        temperature (float): Scaling factor for logits before softmax.\n",
        "        top_k (int, optional): If specified, restricts sampling to top k tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence.\n",
        "    \"\"\"\n",
        "    idx = idx.unsqueeze(0) if idx.dim() == 1 else idx\n",
        "    idx = torch.tensor(idx, device=model.device) if not isinstance(idx, torch.Tensor) else idx.to(model.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Ensure context length does not exceed model's block size\n",
        "        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits, _ = model(idx_cond)\n",
        "\n",
        "        # Extract logits for the last token and apply temperature scaling\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply top-k filtering if necessary\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample next token\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if idx_next == end_token_index:\n",
        "            break\n",
        "        # Append sampled token to sequence\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return decode(idx.tolist()[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOdwJ26Ax2e6"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024 # how many independent sequences will we process in parallel?\n",
        "block_size = 25 # what is the maximum context length for predictions?\n",
        "max_iters = 20000 # CHANGE the step size\n",
        "# num_epochs = 100\n",
        "eval_interval = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "# # torch.manual_seed(1337)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(1337)\n",
        "bias = True # if using bias inside all Linear layers\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "JG9utTPZvaU-",
        "outputId": "cbfb3295-b244-4a36-dae2-f8abf1bd5bf4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250301_234035-7z339az0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0' target=\"_blank\">Trial 6 of finding 90%+ accuracy</a></strong> to <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7abf61f17c50>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"self-improve-transformer-train\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 25,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            },\n",
        "           name = \"Trial 6 of finding 90%+ accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQpX_YZlYluM"
      },
      "outputs": [],
      "source": [
        "wandb.run.notes = \"Trial 5 of finding \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPl7iF7e2D-n"
      },
      "outputs": [],
      "source": [
        "# run this cell if you wanna initialize a new model\n",
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snZoFt8qe76q"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer for base model\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,                  # 学习率 change to 1e-4 from 5e-4\n",
        "    betas=(0.9, 0.99),        # 动量参数\n",
        "    eps=1e-12,                # 数值稳定性参数\n",
        "    weight_decay=0.1          # 权重衰减\n",
        ")\n",
        "\n",
        "total_steps = 20000       # 总训练步数 CHANGE\n",
        "warmup_steps = 1000       # 预热阶段步数\n",
        "decay_steps = 2000        # 衰减阶段步数\n",
        "stable_steps = total_steps - warmup_steps - decay_steps  # 稳定阶段步数\n",
        "\n",
        "# Warmup + Stable + Decay\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        # linear Warmup（0 to base_lr）\n",
        "        return step / warmup_steps\n",
        "    elif step < warmup_steps + stable_steps:\n",
        "        # stable\n",
        "        return 1.0\n",
        "    else:\n",
        "        # Cosine Decay）\n",
        "        decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "        return 0.5 * (1 + math.cos(math.pi * decay_ratio))  # cos 1 to 0\n",
        "\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odtqfBE0swtz",
        "outputId": "d72d51e0-3bfb-4789-f0b3-3c699943ce23"
      },
      "outputs": [],
      "source": [
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "phase = 6\n",
        "counter = 0\n",
        "best_loss = float('inf')\n",
        "val_loss_list = []\n",
        "\n",
        "patience = 20\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "for iter in tqdm(range(max_iters), desc=\"Training Progress\"):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(phase, model)['train']\n",
        "        print(f\"step {iter}: loss {losses:.4f}, Best Loss so far: {best_loss}, Counter: {counter}\")\n",
        "        log_dict = {\"Loss\": losses}\n",
        "        val_loss_list.append(round(losses.item(), 4))\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(phase)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "UzlPUoW3k1zY",
        "outputId": "629100bf-2cc1-440a-ae1e-de021036fa12"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>█▅▄▂▃▁▁▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>1.12336</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Trial 6 of finding 90%+ accuracy</strong> at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train/runs/7z339az0</a><br> View project at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/self-improve-transformer-train</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250301_234035-7z339az0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBC5BDrbf49W"
      },
      "outputs": [],
      "source": [
        "\n",
        "def accuracy_print_one(model, num_digits, need_print=False):\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "\n",
        "        for j in range(100):\n",
        "            a = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "            b = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "            c = a + b\n",
        "            reversed_a = np.array([str(x)[::-1] for x in a])\n",
        "            reversed_b = np.array([str(x)[::-1] for x in b])\n",
        "            reversed_c = np.array([str(x)[::-1] for x in c])\n",
        "            input = f\"{reversed_a.item()}+{reversed_b.item()}=\"\n",
        "            context = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
        "\n",
        "            output1 = generate(model=model, idx=context, max_new_tokens=35, top_k=1)\n",
        "\n",
        "\n",
        "            if output1 == f\"{reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\":\n",
        "                correct += 1\n",
        "                wrong = 0\n",
        "            else:\n",
        "                wrong = 1\n",
        "                \n",
        "            # for testing\n",
        "            if need_print and wrong:\n",
        "                print(f\"   Input: {input}\")\n",
        "                print(f\"  Output: {output1}\")\n",
        "                print(f\"Expected: {reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\")\n",
        "                print(\"-----------\")\n",
        "\n",
        "        acc = correct / 100\n",
        "        print(f\"Accuracy for {num_digits} digits addition: {acc} \")\n",
        "        return acc\n",
        "    \n",
        "def get_avg_performance(models):\n",
        "    dict_acc = {}\n",
        "    for num_dig in range(1, 7):\n",
        "        dict_acc[num_dig] = accuracy_print_one(models, num_dig, need_print=False)\n",
        "    return dict_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6zbg__39wSG",
        "outputId": "4b0f0f43-e36e-48b8-d8a4-98b44f5bb07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 1 digits addition: 1.0 \n",
            "Accuracy for 2 digits addition: 1.0 \n",
            "Accuracy for 3 digits addition: 1.0 \n",
            "Accuracy for 4 digits addition: 1.0 \n",
            "Accuracy for 5 digits addition: 1.0 \n",
            "Accuracy for 6 digits addition: 0.34 \n"
          ]
        }
      ],
      "source": [
        "avg_performance = get_avg_performance(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sJG_uIuVj4P",
        "outputId": "9f7ad6a2-cdcd-475c-8c67-f262c2e7b999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model at /content/drive/MyDrive/URPS/accuracy_0.96_3w_iter.pt\n"
          ]
        }
      ],
      "source": [
        "filename = f\"accuracy_0.96_3w_iter.pt\"\n",
        "save_path = f\"/content/drive/MyDrive/URPS/{filename}\"\n",
        "torch.save(model1.state_dict(), save_path)\n",
        "print(f\"Saved best model at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "Mm-f5OrHgB94",
        "outputId": "cf49a06d-19a4-478a-c14a-cf6644b0f8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.8 \n",
            "Accuracy for 6 digits addition: 0.82 \n",
            "Accuracy for 6 digits addition: 0.87 \n",
            "Accuracy for 6 digits addition: 0.77 \n",
            "Accuracy for 6 digits addition: 0.83 \n",
            "Accuracy for 6 digits addition: 0.75 \n",
            "Accuracy for 6 digits addition: 0.83 \n",
            "Accuracy for 6 digits addition: 0.86 \n",
            "Accuracy for 6 digits addition: 0.82 \n",
            "Accuracy for 6 digits addition: 0.88 \n",
            "Accuracy for 6 digits addition: 0.82 \n",
            "Accuracy for 6 digits addition: 0.82 \n",
            "Accuracy for 6 digits addition: 0.87 \n",
            "Accuracy for 6 digits addition: 0.83 \n",
            "Accuracy for 6 digits addition: 0.8 \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-dc01d27e19e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0macc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_print_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b375d9553097>\u001b[0m in \u001b[0;36maccuracy_print_one\u001b[0;34m(model, num_digits, need_print)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-6cdde7e6eb40>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, idx, max_new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Forward pass to get logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Extract logits for the last token and apply temperature scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15f1b2a21e8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# + pos_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15f1b2a21e8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Apply residual connection and pre-normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply LayerNorm before attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply LayerNorm before MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15f1b2a21e8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Reshape back to original format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reassemble heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Output projection and residual dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "acc_list = []\n",
        "for i in range(100):\n",
        "    acc_list.append(accuracy_print_one(model1, 6, need_print=False))\n",
        "print(sum(acc_list)/len(acc_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dmPenbJQC4J"
      },
      "outputs": [],
      "source": [
        "def test_accuracy_on_6(model):\n",
        "    return accuracy_print_one(model, 6, need_print=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj9N9BGSQDoX"
      },
      "outputs": [],
      "source": [
        "def create_optimizer_and_scheduler(model):\n",
        "    # AdamW\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,              # learning rate\n",
        "        betas=(0.9, 0.99),\n",
        "        eps=1e-12,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # LR Scheduler\n",
        "    total_steps = 20000 # change to 30000\n",
        "    warmup_steps = 1000\n",
        "    decay_steps = 2000\n",
        "    stable_steps = total_steps - warmup_steps - decay_steps\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps  # Linear warmup 0->1\n",
        "        elif step < warmup_steps + stable_steps:\n",
        "            return 1.0                  # Stable\n",
        "        else:\n",
        "            # Cosine decay from 1->0\n",
        "            decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "            return 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "q62fWUVdQHgV",
        "outputId": "733985d5-4b07-45b5-bc69-8ad5df31ef33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start run 1/5\n",
            "10.658304 M parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   0%|          | 0/20000 [00:03<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: loss 2.6712, Best Loss so far: inf, Counter: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-c6886c88453d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mval_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# sample a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "num_runs = 5\n",
        "threshold = 0.85\n",
        "\n",
        "for run_idx in range(num_runs):\n",
        "    print(f\"Start run {run_idx + 1}/{num_runs}\")\n",
        "    # initialize model, optimizer, scheduler\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "    m = model.to(device)\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(model)\n",
        "\n",
        "    # training loop\n",
        "\n",
        "    # print the number of parameters in the model\n",
        "    print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "    phase = 6\n",
        "    counter = 0\n",
        "    best_loss = float('inf')\n",
        "    val_loss_list = []\n",
        "\n",
        "    patience = 20\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "    for iter in tqdm(range(max_iters), desc=\"Training Progress\"):\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "            losses1 = estimate_loss(phase, model)['train']\n",
        "            print(f\"step {iter}: loss {losses1:.4f}, Best Loss so far: {best_loss}, Counter: {counter}\")\n",
        "            log_dict = {\"Loss\": losses1}\n",
        "            val_loss_list.append(round(losses1.item(), 4))\n",
        "\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb = get_batch(phase)\n",
        "\n",
        "        # evaluate the loss\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "            logits1, loss1 = model(xb, yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        scaler.scale(loss1).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        scheduler.step()\n",
        "    print(f\"Training finished for run {run_idx+1}.\\nEvaluating 6-digit accuracy...\")\n",
        "\n",
        "    # 8) Evaluate final performance on 6-digit addition\n",
        "    acc_on_6 = test_accuracy_on_6(model)\n",
        "\n",
        "    # 9) If it's good enough, save to Google Drive (or local)\n",
        "    if acc_on_6 > threshold:\n",
        "        filename = f\"run_{run_idx}_acc_{acc_on_6:.2f}.pt\"\n",
        "        save_path = f\"/content/drive/MyDrive/URPS/{filename}\"\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Saved best model at {save_path}\")\n",
        "\n",
        "print(\"All runs complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf-KW9gJiu6g"
      },
      "outputs": [],
      "source": [
        "def accuracy_print(models, num_digits, need_print=False):\n",
        "        correct = 0\n",
        "        wrong = 0\n",
        "\n",
        "        for j in range(100):\n",
        "            a = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "            b = np.random.randint(10**(num_digits-1), 10**(num_digits), size=1)\n",
        "            c = a + b\n",
        "            reversed_a = np.array([str(x)[::-1] for x in a])\n",
        "            reversed_b = np.array([str(x)[::-1] for x in b])\n",
        "            reversed_c = np.array([str(x)[::-1] for x in c])\n",
        "            input = f\"{reversed_a.item()}+{reversed_b.item()}=\"\n",
        "            context = torch.tensor(encode(input), dtype=torch.long, device=device)\n",
        "\n",
        "            output1 = generate(models[0], context, 100, top_k=1)\n",
        "            output2 = generate(models[1], context, 100, top_k=1)\n",
        "            output3 = generate(models[2], context, 100, top_k=1)\n",
        "            output4 = generate(models[3], context, 100, top_k=1)\n",
        "            output5 = generate(models[4], context, 100, top_k=1)\n",
        "            output = [output1, output2, output3, output4, output5]\n",
        "\n",
        "            counter = Counter(output)\n",
        "            # 选取出现次数最多的字符串\n",
        "            most_common_o, count = counter.most_common(1)[0]\n",
        "            # print(\"most common output count:\", count)\n",
        "\n",
        "            if most_common_o == f\"{reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\":\n",
        "                correct += 1\n",
        "                wrong = 0\n",
        "            else:\n",
        "                wrong = 1\n",
        "            if need_print and wrong:\n",
        "                print(f\"   Input: {input}\")\n",
        "                print(f\"  Output: {most_common_o}\")\n",
        "                print(f\"Expected: {reversed_a.item()}+{reversed_b.item()}={reversed_c.item()}\")\n",
        "                print(\"-----------\")\n",
        "\n",
        "        acc = correct / 100\n",
        "        print(f\"Accuracy for {num_digits} digits addition: {acc} \")\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23jaqlhaaOGQ"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "os.system('git config --global user.email \"zifeibai@umich.edu\"')\n",
        "os.system('git config --global user.name \"ZifeiBai\"')\n",
        "\n",
        "# 2️⃣ **Use Google Drive to store GitHub Token**\n",
        "GITHUB_TOKEN_PATH = \"/content/drive/MyDrive/URPS/github_token.txt\"\n",
        "if os.path.exists(GITHUB_TOKEN_PATH):\n",
        "    with open(GITHUB_TOKEN_PATH, \"r\") as f:\n",
        "        os.environ[\"GITHUB_TOKEN\"] = f.read().strip()\n",
        "else:\n",
        "    print(\"❌ GitHub Token\")\n",
        "    exit(1)\n",
        "\n",
        "# 3️⃣ **Set up GitHub remote repo**\n",
        "GIT_PATH = \"/content/drive/MyDrive/URPS/Git\"\n",
        "REPO_URL = f\"https://{os.environ['GITHUB_TOKEN']}@github.com/ZifeiBai/URPS.git\"\n",
        "\n",
        "if not os.path.exists(GIT_PATH):\n",
        "    print(f\"📁 Creating directory: {GIT_PATH}\")\n",
        "    os.makedirs(GIT_PATH)\n",
        "\n",
        "# 4️⃣ **If .git/ does not exsit， need to clone**\n",
        "if not os.path.exists(os.path.join(GIT_PATH, \".git\")):\n",
        "    print(\"❌ Git repository not found. Cloning...\")\n",
        "    subprocess.run(f\"rm -rf {GIT_PATH}\", shell=True, check=True)\n",
        "    subprocess.run(f\"git clone {REPO_URL} {GIT_PATH}\", shell=True, check=True)\n",
        "\n",
        "# 5️⃣ **Enter Git repo**\n",
        "os.chdir(GIT_PATH)\n",
        "print(\"📂 Changed working directory to:\", os.getcwd())\n",
        "\n",
        "\n",
        "# 6️⃣ **Check Git status**\n",
        "status_output = subprocess.run(\"git status\", shell=True, capture_output=True, text=True)\n",
        "print(status_output.stdout)\n",
        "\n",
        "#  **Push to Git**\n",
        "print(\"🚀 Adding files to Git...\")\n",
        "subprocess.run(\"git add .\", shell=True, check=True)\n",
        "\n",
        "print(\"📝 Committing changes...\")\n",
        "commit_output = subprocess.run('git commit -m \"Auto update from Google Colab 2.6\"', shell=True, capture_output=True, text=True)\n",
        "print(commit_output.stdout)\n",
        "\n",
        "\n",
        "\n",
        "print(\"📤 Pushing to GitHub...\")\n",
        "push_output = subprocess.run(\"git push origin main\", shell=True, capture_output=True, text=True)\n",
        "if \"fatal\" in push_output.stderr or \"error:\" in push_output.stderr:\n",
        "    print(\"❌ Real Git Push Error:\", push_output.stderr)\n",
        "else:\n",
        "    print(\"✅ Git Push Success!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQIIfc4TeXFn",
        "outputId": "bead26d3-013b-4249-9c50-15e6128e72b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(14, 384)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
              "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=14, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model.to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode.\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB7DwhH_eXu4",
        "outputId": "232a8af7-c956-4e96-84ca-c5bd8402da05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 1 digits addition: 1.0 \n",
            "Accuracy for 2 digits addition: 1.0 \n",
            "Accuracy for 3 digits addition: 1.0 \n",
            "Accuracy for 4 digits addition: 1.0 \n",
            "Accuracy for 5 digits addition: 1.0 \n",
            "Accuracy for 6 digits addition: 0.92 \n"
          ]
        }
      ],
      "source": [
        "avg_performance = get_avg_performance(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaDulsThefeF",
        "outputId": "095c26c4-d263-4f3c-994b-03c6f19c8f82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-70-96bf2cf3a135>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.89 \n",
            "Accuracy on 6-digit addition of base model: 0.89\n"
          ]
        }
      ],
      "source": [
        "model1 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model1.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/accuracy_0.96_3w_iter.pt\"\n",
        "model1.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model1.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model1)\n",
        "print(f\"Accuracy on 6-digit addition of base model: {acc_on_6}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRwP9XlgJjAR"
      },
      "outputs": [],
      "source": [
        "# Helper func\n",
        "def generate_prompt_OOD(phase):\n",
        "    \"\"\"\n",
        "    Return a list of 'num_prompts' strings for reversed addition\n",
        "    with 'operand_length' digits each.\n",
        "    \"\"\"\n",
        "    a = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    b = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    prompt_str = f\"{str(a)[::-1]}+{str(b)[::-1]}=\"  # e.g. '12345+54321='\n",
        "\n",
        "    return prompt_str\n",
        "\n",
        "def length_filter(predictions, threshold=threshold):\n",
        "    \"\"\"\n",
        "    Filter out predictions that are shorter than the threshold.\n",
        "    \"\"\"\n",
        "    max_len = max(len(pred) for pred in predictions) # 7\n",
        "    min_len = max_len - threshold # 6\n",
        "    filtered = [(pred) for pred in predictions if len(pred) >= min_len]\n",
        "\n",
        "    # print(f\"Filtered out {len(predictions) - len(filtered)} short outputs; kept {len(filtered)}.\")\n",
        "    return filtered\n",
        "\n",
        "def pad_sequence(seq_tensor, block_size, padding_idx):\n",
        "        \"\"\"\n",
        "        Pad (or truncate) a single 1D seq_tensor to length block_size.\n",
        "        \"\"\"\n",
        "        seq_list = seq_tensor.tolist()\n",
        "        if len(seq_list) < block_size:\n",
        "            seq_list += [padding_idx] * (block_size - len(seq_list))\n",
        "        else:\n",
        "            seq_list = seq_list[:block_size]\n",
        "        return torch.tensor(seq_list, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNS5h6FmSkl8"
      },
      "outputs": [],
      "source": [
        "# length filter per model, return xb, yb with shape (filtered_batch_size, 24)\n",
        "def generate_length_filter_OOD(\n",
        "    model,\n",
        "    num_samples=100,\n",
        "    batch_size=1024,\n",
        "    phase=6,\n",
        "    threshold=1,\n",
        "    max_new_tokens=35,\n",
        "    stoi=stoi,\n",
        "    itos=itos,\n",
        "    device='cuda',\n",
        "    block_size=25,\n",
        "    padding_idx=padding_token_index,\n",
        "    eos_idx=end_token_index\n",
        "):\n",
        "\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    for _ in tqdm(range(num_batches), desc=\"for each batch\"): # for each batch\n",
        "\n",
        "        prompts = [generate_prompt_OOD(phase) for _ in range(batch_size)] # prompts of size of batch_size=1024\n",
        "\n",
        "        batch_predictions = []\n",
        "        for prompt_str in prompts:\n",
        "            prompt_ids = [stoi[ch] for ch in prompt_str]\n",
        "            prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long, device=device)\n",
        "\n",
        "            # single model generate predictions for each prompt\n",
        "            out_str = generate(\n",
        "                model=model,\n",
        "                idx=prompt_tensor,\n",
        "                max_new_tokens=35,\n",
        "                top_k=1\n",
        "            )\n",
        "\n",
        "            out_ids = [stoi[ch] for ch in out_str if ch in stoi]\n",
        "\n",
        "            # Add ending_tokens for future training\n",
        "            if eos_idx is not None:\n",
        "                out_ids.append(eos_idx)\n",
        "\n",
        "            # Convert to tensor\n",
        "            out_tensor = torch.tensor(out_ids, dtype=torch.long, device=device)\n",
        "            batch_predictions.append(out_tensor)\n",
        "\n",
        "        # length filtering on this batch\n",
        "        filtered_batch = length_filter(batch_predictions, threshold=threshold)\n",
        "        if not filtered_batch:\n",
        "            continue\n",
        "        # print(filtered_batch)\n",
        "        # Pad/Truncate each sequence\n",
        "        batch_encoded = [pad_sequence(seq, block_size, padding_idx) for seq in filtered_batch]\n",
        "\n",
        "        # Build (x, y) by shifting\n",
        "        xb = torch.stack([seq[:-1] for seq in batch_encoded])  # [B, block_size-1]\n",
        "        yb = torch.stack([seq[1:] for seq in batch_encoded])   # [B, block_size-1]\n",
        "\n",
        "        all_x.append(xb)\n",
        "        all_y.append(yb)\n",
        "\n",
        "    if not all_x:\n",
        "        return None, None\n",
        "\n",
        "    final_x = torch.cat(all_x, dim=0).to(device)\n",
        "    final_y = torch.cat(all_y, dim=0).to(device)\n",
        "    return final_x, final_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3aMpDXFrllU",
        "outputId": "bdef2c5a-4b19-447c-c70d-c73c2a0b2e77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-129-ff04680666f8>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model1.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.9 \n",
            "Accuracy on 6-digit addition of base model: 0.9\n"
          ]
        }
      ],
      "source": [
        "model1 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model1.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/accuracy_0.96_3w_iter.pt\"\n",
        "model1.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model1.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wafRWVgrzNK",
        "outputId": "37e8eb8c-70c7-44e3-d009-6c88e07775e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-132-6fce3ffa9d5f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model2.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.91 \n"
          ]
        }
      ],
      "source": [
        "model2 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model2.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/model.pth\"\n",
        "model2.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model2.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJk1keP7r-Oe",
        "outputId": "61d12564-d091-4971-f115-6bfdda1c180c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-131-10d7336150d7>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model3.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 6 digits addition: 0.88 \n"
          ]
        }
      ],
      "source": [
        "model3 = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "model3.to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/URPS/Models/run_0_acc_0.95.pt\"\n",
        "model3.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model3.eval()\n",
        "acc_on_6 = test_accuracy_on_6(model3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6flQf2vsjmM"
      },
      "outputs": [],
      "source": [
        "models = [model1, model2, model3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj3Kp1l8ufFJ"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def generate_prompt_OOD(phase):\n",
        "    \"\"\"\n",
        "    Return a list of 'num_prompts' strings for reversed addition\n",
        "    with 'operand_length' digits each.\n",
        "    \"\"\"\n",
        "    a = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    b = np.random.randint(10**(phase-1), 10**phase - 1)\n",
        "    prompt_str = f\"{str(a)[::-1]}+{str(b)[::-1]}=\"  # e.g. '12345+54321='\n",
        "\n",
        "    return prompt_str\n",
        "\n",
        "def length_filter(predictions, threshold=threshold):\n",
        "    \"\"\"\n",
        "    Filter out predictions that are shorter than the threshold.\n",
        "    \"\"\"\n",
        "    max_len = max(len(pred) for pred in predictions) # 7\n",
        "    min_len = max_len - threshold # 6\n",
        "    filtered = [(pred) for pred in predictions if len(pred) >= min_len]\n",
        "\n",
        "    # print(f\"Filtered out {len(predictions) - len(filtered)} short outputs; kept {len(filtered)}.\")\n",
        "    return filtered\n",
        "\n",
        "def pad_sequence(seq_tensor, block_size, padding_idx):\n",
        "        \"\"\"\n",
        "        Pad (or truncate) a single 1D seq_tensor to length block_size.\n",
        "        \"\"\"\n",
        "        seq_list = seq_tensor.tolist()\n",
        "        if len(seq_list) < block_size:\n",
        "            seq_list += [padding_idx] * (block_size - len(seq_list))\n",
        "        else:\n",
        "            seq_list = seq_list[:block_size]\n",
        "        return torch.tensor(seq_list, dtype=torch.long)\n",
        "\n",
        "def majority_vote_filter(pred_tensors, vote_threshold=0.6):\n",
        "    \"\"\"\n",
        "    pred_tensors: list of Tensors, each is 1D token IDs, all passed length filter.\n",
        "    vote_threshold: e.g. 0.6 => need at least ceil(0.6*k) identical strings\n",
        "    Returns best_tensor if there's a winning string, else None.\n",
        "    \"\"\"\n",
        "    if not pred_tensors:\n",
        "        return None\n",
        "\n",
        "    num_models = len(pred_tensors)\n",
        "    needed_votes = math.ceil(vote_threshold * num_models) # ceil(5 * 0.6) = 3, ceil(3 * 0.6) = 2\n",
        "\n",
        "    # decode each tensor -> string\n",
        "    string_list = []\n",
        "    for t in pred_tensors:\n",
        "        string_list.append(decode(t.tolist())) # string_list contains all the predicted strings\n",
        "\n",
        "    freq = {}\n",
        "    for s in string_list:\n",
        "        freq[s] = freq.get(s, 0) + 1\n",
        "\n",
        "    # find most frequent string\n",
        "    # best_str is the most frequent string\n",
        "    # best_count is how many times it appears\n",
        "    best_str, best_count = None, 0\n",
        "    for text, count in freq.items():\n",
        "        if count > best_count:\n",
        "            best_str = text\n",
        "            best_count = count\n",
        "\n",
        "    # compare the frequency with the threshold\n",
        "    if best_count >= needed_votes:\n",
        "        best_ids = encode(best_str)\n",
        "        return torch.tensor(best_ids, dtype=torch.long)\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB7L1OixsSu5"
      },
      "outputs": [],
      "source": [
        "def generate_multi_model_length_vote_with_counter(\n",
        "    models,                # list of models [M1, M2, ...]\n",
        "    num_samples=100,\n",
        "    batch_size=1024,\n",
        "    phase=6,                 # digit length for reversed addition\n",
        "    block_size=25,\n",
        "    padding_idx=padding_token_index,\n",
        "    max_new_tokens=50,\n",
        "    threshold=1,         # length-filter threshold\n",
        "    vote_threshold=0.6,    # 5 model 选 3 model\n",
        "    eos_idx=end_token_index,\n",
        "    device=device\n",
        "):\n",
        "\n",
        "    all_x = []\n",
        "    all_y = []\n",
        "\n",
        "    num_models = len(models)\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "    kept_count = 0\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        # generate 'batch_size' prompts of digit length 6\n",
        "        prompts = [generate_prompt_OOD(phase) for _ in range(batch_size)]\n",
        "\n",
        "        for prompt_str in prompts: # iterate through all 1024 prompts\n",
        "\n",
        "            # encode and convert prompt_str into tensor\n",
        "            prompt_ids = encode(prompt_str)\n",
        "            prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long, device=device)\n",
        "\n",
        "            # collect each model's output for this prompt\n",
        "            model_outputs = []\n",
        "\n",
        "            for model_s in models:\n",
        "                out_str = generate(\n",
        "                    model=model_s,\n",
        "                    idx=prompt_tensor,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    top_k=1\n",
        "                )\n",
        "                # re-encode the generated string\n",
        "                out_ids = encode(out_str)\n",
        "                if eos_idx is not None:\n",
        "                    out_ids.append(eos_idx)\n",
        "\n",
        "                out_tensor = torch.tensor(out_ids, dtype=torch.long, device=device)\n",
        "                model_outputs.append(out_tensor) # len(model_outputs) == number of models\n",
        "\n",
        "            # length filter\n",
        "            # remove any model outputs that length < 6\n",
        "            filtered = length_filter(model_outputs, threshold=1)\n",
        "            if not filtered:\n",
        "                continue\n",
        "\n",
        "            # majority vote\n",
        "            best_tensor = majority_vote_filter(filtered, vote_threshold=0.6)\n",
        "            if best_tensor is None:\n",
        "                continue\n",
        "\n",
        "            # pad/truncate\n",
        "            seq_padded = pad_sequence(best_tensor, block_size, padding_idx)\n",
        "\n",
        "            # build (x, y)\n",
        "            xb = seq_padded[:-1]\n",
        "            yb = seq_padded[1:]\n",
        "            all_x.append(xb)\n",
        "            all_y.append(yb)\n",
        "            kept_count += 1\n",
        "\n",
        "    final_x = torch.stack(all_x, dim=0).to(device)\n",
        "    final_y = torch.stack(all_y, dim=0).to(device)\n",
        "\n",
        "    print(f\"Total final pairs kept: {kept_count}\")\n",
        "    return final_x, final_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UxRfkUErst5Z",
        "outputId": "96449990-89c0-4666-8fbf-0858183656af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total final pairs kept: 49314\n"
          ]
        }
      ],
      "source": [
        "x, y = generate_multi_model_length_vote_with_counter(models, num_samples=50000)\n",
        "self_improvement_data_6_digits = {\n",
        "    \"xb\": x,\n",
        "    \"yb\": y\n",
        "}\n",
        "save_path = \"/content/drive/MyDrive/URPS/Data/self_improvement_data_6_digits.pt\"\n",
        "torch.save(self_improvement_data_6_digits, save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CicHFfM_1LDc",
        "outputId": "aa265e65-bf09-4c06-f90b-a09d348a8e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 8,  5,  1,  ..., 13, 13, 13],\n",
            "        [ 9,  4,  9,  ..., 12, 13, 13],\n",
            "        [ 4,  9,  2,  ..., 13, 13, 13],\n",
            "        ...,\n",
            "        [ 6,  7,  8,  ..., 13, 13, 13],\n",
            "        [ 3,  1,  2,  ..., 13, 13, 13],\n",
            "        [ 4,  0,  5,  ..., 12, 13, 13]], device='cuda:0')\n",
            "tensor([[ 5,  1,  5,  ..., 13, 13, 13],\n",
            "        [ 4,  9,  5,  ..., 13, 13, 13],\n",
            "        [ 9,  2,  8,  ..., 13, 13, 13],\n",
            "        ...,\n",
            "        [ 7,  8,  0,  ..., 13, 13, 13],\n",
            "        [ 1,  2,  2,  ..., 13, 13, 13],\n",
            "        [ 0,  5,  4,  ..., 13, 13, 13]], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-193-e39e954e4e71>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(\"/content/drive/MyDrive/URPS/Data/self_improvement_data_6_digits.pt\")\n"
          ]
        }
      ],
      "source": [
        "data = torch.load(\"/content/drive/MyDrive/URPS/Data/self_improvement_data_6_digits.pt\")\n",
        "xb = data[\"xb\"]\n",
        "yb = data[\"yb\"]\n",
        "print(xb)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRB1Uj3cU9Aw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
