{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhYBYz_Yj1yq",
        "outputId": "0e29d9d0-2afc-4b6b-e57a-183af753d137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2jemn0ZkDm-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import math\n",
        "import random\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIM_06ltkW26",
        "outputId": "33792009-e144-426b-a637-0f1123d0ad78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 10, 3, 11]\n",
            "12=3&\n"
          ]
        }
      ],
      "source": [
        "vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '&', '*']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "padding_token_index = 12\n",
        "end_token_index = 11\n",
        "\n",
        "# create a mapping from chars to ints\n",
        "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
        "itos = {i:ch for i, ch in enumerate(vocab)}\n",
        "encode = lambda s:[stoi[c] for c in s] # encoder: take a string, output a list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of ints, output a string\n",
        "\n",
        "print(encode(\"12=3&\"))\n",
        "print(decode(encode(\"12=3&\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUKn-XQwwfzt"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000 # how many independent sequences will we process in parallel?\n",
        "block_size = 60 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # CHANGE the step size\n",
        "eval_interval = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "bias = True\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UscFoJ1JkdRU"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias=False): # class constructor\n",
        "        super().__init__()\n",
        "        # nn.Parameter, pytorch optimize will update the value of this parameter during training\n",
        "        self.weight = nn.Parameter(torch.ones(ndim)) # trainable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # trainable parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Key, Query, Value projections\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=bias)\n",
        "\n",
        "        # T-5 PE\n",
        "        # self.rel_pos_bias = T5RelativePositionBias(block_size, n_head)\n",
        "\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "                # Check for Flash Attention availability\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # Causal mask for slow attention\n",
        "            self.register_buffer(\n",
        "                \"bias\",\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch size, sequence length, embedding dimension\n",
        "\n",
        "        # Compute Q, K, V\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # Split into Q, K, V (B, T, n_embd)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T, head_size)\n",
        "\n",
        "        # Compute T5 relative position bias\n",
        "        # self.rel_pos_bias = self.rel_pos_bias.to(device)  # Move to correct device\n",
        "        # rel_bias = self.rel_pos_bias(T, device)  # Compute relative position bias\n",
        "        # (1, num_heads, T, T)\n",
        "\n",
        "        # Flash Attention or fallback to manual implementation\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v,\n",
        "                attn_mask=None,\n",
        "                dropout_p=self.dropout if self.training else 0,\n",
        "                is_causal=True\n",
        "            )\n",
        "        # else:\n",
        "        # Manual attention with causal masking\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  # Scaled dot product\n",
        "        # # att = att + rel_bias  # Apply relative positional bias\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))  # Apply causal mask\n",
        "        # att = F.softmax(att, dim=-1)  # Normalize attention scores\n",
        "        # att = self.attn_dropout(att)\n",
        "        # y = att @ v  # Apply attention weights to values (B, n_head, T, head_size)\n",
        "\n",
        "        # Reshape back to original format\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Reassemble heads\n",
        "\n",
        "        # Output projection and residual dropout\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# SwiGLU used in llama\n",
        "class SwiGLUFFN(nn.Module):\n",
        "    def __init__(self, n_embd: int, dropout: float = 0.0, bias: bool = False):\n",
        "        super().__init__()\n",
        "        d_ff = int((8/3) * n_embd)\n",
        "        self.fc1 = nn.Linear(n_embd, 2 * d_ff, bias=bias)\n",
        "        self.fc2 = nn.Linear(d_ff, n_embd, bias=bias)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_proj = self.fc1(x)\n",
        "        x1, x2 = x_proj.chunk(2, dim=-1)\n",
        "        swish = x1 * torch.sigmoid(x1)\n",
        "        x = swish * x2\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, dropout, block_size, bias=True):\n",
        "        super().__init__()\n",
        "        # LayerNorm and CausalSelfAttention with explicit parameters\n",
        "        self.ln_1 = LayerNorm(n_embd, bias=bias)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout, block_size, bias=bias)\n",
        "        self.ln_2 = LayerNorm(n_embd, bias=bias)\n",
        "        # self.mlp = MLP(n_embd, dropout, bias=bias)  # MLP with explicit parameters\n",
        "        self.mlp = SwiGLUFFN(n_embd, dropout) #bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply residual connection and pre-normalization\n",
        "        x = x + self.attn(self.ln_1(x))  # Apply LayerNorm before attention\n",
        "        x = x + self.mlp(self.ln_2(x))  # Apply LayerNorm before MLP\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=True):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        super().__init__()\n",
        "        assert vocab_size is not None\n",
        "        assert block_size is not None\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.dropout = dropout\n",
        "        self.bias = bias\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(vocab_size, n_embd), # token embeddings\n",
        "            # wpe = nn.Embedding(block_size, n_embd), # positional embeddings CHANGE, t-5 positional embedding\n",
        "            drop = nn.Dropout(dropout),\n",
        "            h = nn.ModuleList([Block(n_embd, n_head, dropout, block_size, bias=bias) for _ in range(n_layer)]), # a stack of n_layer blocks\n",
        "            ln_f = LayerNorm(n_embd, bias=bias), # final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False) # projects the final transformer output to the vocab size\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        # pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb)# + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=encode(\"*\")[0])\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            # loss = None\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx887ztRk-9U"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, max_new_tokens, temperature=0.00001, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate a sequence of tokens given an initial sequence.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model used for generation.\n",
        "        idx (torch.Tensor or list): Initial sequence of indices (LongTensor of shape (b,t)).\n",
        "        max_new_tokens (int): Number of new tokens to generate.\n",
        "        temperature (float): Scaling factor for logits before softmax.\n",
        "        top_k (int, optional): If specified, restricts sampling to top k tokens.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence.\n",
        "    \"\"\"\n",
        "    #idx = idx.unsqueeze(0) if idx.dim() == 1 else idx\n",
        "    #idx = torch.tensor(idx, device=model.device) if not isinstance(idx, torch.Tensor) else idx.to(model.device)\n",
        "    batch_size, seq_len = idx.shape\n",
        "    idx = idx.to(model.device)\n",
        "\n",
        "    # Track which sequences are still active (not finished)\n",
        "    is_active = torch.ones(batch_size, dtype=torch.bool, device=model.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if not is_active.any():\n",
        "            break\n",
        "        # Ensure context length does not exceed model's block size\n",
        "        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits, _ = model(idx_cond)\n",
        "\n",
        "        # Extract logits for the last token and apply temperature scaling\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply top-k filtering if necessary\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)), dim=-1)\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Convert logits to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample next token\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if is_active[i] and idx_next[i].item() == encode('&')[0]:\n",
        "                is_active[i] = False  # if \"&\" appears, stop generating\n",
        "\n",
        "        # Stop if all sequences have reached `end_token_index`\n",
        "        if not is_active.any():\n",
        "            break\n",
        "\n",
        "        # Append sampled token to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    decoded_texts = []\n",
        "    for seq in idx.tolist():\n",
        "        text = decode(seq)\n",
        "        cut_text = text.split('&')[0]  # make sure generate tokens don't have \"&\", only got tokens before \"&\"\n",
        "        decoded_texts.append(cut_text)\n",
        "\n",
        "    return decoded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFz93vUdlEy8"
      },
      "outputs": [],
      "source": [
        "def generate_origin_dataset(original, task, num_samples = 2000000):\n",
        "    file_path = f\"/content/drive/MyDrive/URPS/Data/origin_ds_{task}.txt\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"File {file_path} already exists.\\nSkipping generation.\")\n",
        "        return\n",
        "    if task == 'copy':\n",
        "        # generate 200000 sample\n",
        "        a_values = np.random.randint(1, original + 1, size=num_samples)\n",
        "        strings = [\"\".join(np.random.choice([str(i) for i in range(10)], size=a)) for a in a_values]  # random generate strings\n",
        "        target = strings\n",
        "        to_write = [f\"{a}={b}&\" for a, b in zip(strings, target)]\n",
        "\n",
        "        # write down\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(\"\\n\".join(to_write))\n",
        "\n",
        "    print(f\"{num_samples} original data for task {task} is saved in {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyYepnmulMwi"
      },
      "outputs": [],
      "source": [
        "# create 50000 OOD data, save\n",
        "def generate_prompt_OOD(si_round, task, original):\n",
        "    \"\"\"\n",
        "    Return a list of 'num_prompts' strings for task\n",
        "    with 'original+si_round' digits each.\n",
        "    \"\"\"\n",
        "    if task == 'copy':\n",
        "        strings = \"\".join(np.random.choice([str(i) for i in range(10)], size=si_round+original))\n",
        "        prompt_str = f\"{str(strings)}=\"  # e.g. '1235455='\n",
        "\n",
        "    return prompt_str\n",
        "\n",
        "\n",
        "def gen_si_data(model, si_round, task, num_samples=100000, block_size=block_size, batch_size=batch_size): # length filtering\n",
        "    output_path = f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_round-1}.txt\"\n",
        "    num_batches = (num_samples) // batch_size + 1\n",
        "    print(f\"Generating {si_round} si data...\")\n",
        "    for _ in range(num_batches):\n",
        "        # generate 'batch_size' prompts of digit length (original + si_round)\n",
        "        prompts = [generate_prompt_OOD(si_round, task, original=10) for _ in range(batch_size)]\n",
        "        encoded_prompts = []\n",
        "\n",
        "        for prompt_str in prompts: # iterate through all 1000 prompts\n",
        "            # encode and convert prompt_str into tensor\n",
        "            prompt_ids = encode(prompt_str)\n",
        "            encoded_prompts.append(prompt_ids)  # Add encoded prompt to the list\n",
        "\n",
        "        prompt_tensor = torch.tensor(encoded_prompts, dtype=torch.long, device=device)\n",
        "        out_str = generate(\n",
        "            model=model,\n",
        "            idx=prompt_tensor,\n",
        "            max_new_tokens=35,\n",
        "            top_k=1\n",
        "        )\n",
        "\n",
        "        # length filter\n",
        "        out_str = [text for text in out_str if len(text[(si_round+11):]) == (si_round + 10)]\n",
        "\n",
        "        # print(len(out_str[0]))\n",
        "        # print(out_str)\n",
        "        # check number of lines in this file\n",
        "        if os.path.exists(output_path):\n",
        "            with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                current_lines = sum(1 for _ in f)\n",
        "        else:\n",
        "            current_lines = 0\n",
        "\n",
        "        # If we already have 50,000 lines, stop\n",
        "        if current_lines >= 50000:\n",
        "            print(f\"Already reached 50,000 lines. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        # calculate remaining lines\n",
        "        remaining = max(0, 50000 - current_lines)  # Prevent negative values\n",
        "        to_write = out_str[:remaining]  # Only write needed amount\n",
        "\n",
        "\n",
        "        # append write down\n",
        "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.writelines([line + \"&\\n\" for line in to_write])\n",
        "\n",
        "        # if 50000 rows, break\n",
        "        # if len(to_write) < batch_size:\n",
        "        #     break\n",
        "\n",
        "    print(f\"Writing complete. \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7vu2D-7lVCh"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, batch_size=batch_size, block_size=block_size):\n",
        "    \"\"\"data is combined dataset, get combined dataset in train loop\"\"\"\n",
        "    final_sample = random.sample(data, batch_size)\n",
        "    final_sample = [line.strip() for line in final_sample]\n",
        "\n",
        "    x_list, y_list = [], []\n",
        "    for x_str in final_sample:\n",
        "        # print(x_str)\n",
        "        x_encoded = encode(x_str)\n",
        "        x_padded = x_encoded + [padding_token_index] * (block_size - len(x_encoded))\n",
        "        x_list.append(torch.tensor(x_padded, dtype=torch.int64))\n",
        "        y_encoded = encode(x_str)[1:]\n",
        "        y_encoded.append(end_token_index)\n",
        "        y_padded = y_encoded + [padding_token_index] * (block_size - len(y_encoded))\n",
        "        y_list.append(torch.tensor(y_padded, dtype=torch.int64))\n",
        "\n",
        "    x_tensor = torch.stack(x_list).to(device)\n",
        "    y_tensor = torch.stack(y_list).to(device)\n",
        "    return x_tensor, y_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PQla4tnS7Zx"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdzGXrYaS418",
        "outputId": "fb0e9b67-8a8b-40cb-8843-e53ec6ce9f23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 60])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "get_batch(data)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJPFkJQIklWy"
      },
      "outputs": [],
      "source": [
        "eval_iters = 100\n",
        "@torch.no_grad()\n",
        "def estimate_loss(data, model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "        X, Y = get_batch(data)\n",
        "        padding_mask_x = (X != padding_token_index).long()\n",
        "        logits, loss = model(X, Y)\n",
        "        losses[k] = loss.item()\n",
        "    out['loss'] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G1kmYoelY9I"
      },
      "outputs": [],
      "source": [
        "# Helper function for multiple training models for 90%+ accuracy\n",
        "def create_optimizer_and_scheduler(model, total, warm, decay):\n",
        "    # AdamW\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,              # learning rate\n",
        "        betas=(0.9, 0.99),\n",
        "        eps=1e-12,\n",
        "        weight_decay=0.1\n",
        "    )\n",
        "\n",
        "    # LR Scheduler\n",
        "    total_steps = total # CHANGE, CHECK max_iter\n",
        "    warmup_steps = warm\n",
        "    decay_steps = decay\n",
        "    stable_steps = total_steps - warmup_steps - decay_steps\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps  # Linear warmup 0->1\n",
        "        elif step < warmup_steps + stable_steps:\n",
        "            return 1.0                  # Stable\n",
        "        else:\n",
        "            # Cosine decay from 1->0\n",
        "            decay_ratio = (step - warmup_steps - stable_steps) / decay_steps\n",
        "            return 0.5 * (1 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "    return optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sT_PR1oD0fP"
      },
      "outputs": [],
      "source": [
        "# Helper function for accuracy printing for each model\n",
        "def accuracy_print_one(model, num_digits, need_print=False):\n",
        "    correct = 0\n",
        "    total = 1000\n",
        "    num_batches = total // batch_size\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        prompts = [\"\".join(np.random.choice([str(i) for i in range(10)], size=num_digits)) + \"=\" for _ in range(batch_size)]  # random generate strings\n",
        "\n",
        "        context = torch.tensor([encode(inp) for inp in prompts], dtype=torch.long, device=device)\n",
        "\n",
        "        # output in batch\n",
        "        output_batch = generate(model=model, idx=context, max_new_tokens=35, top_k=1)\n",
        "\n",
        "        targets = [p + p[:-1] for p in prompts]\n",
        "        correct += sum([output == target for output, target in zip(output_batch, targets)])\n",
        "\n",
        "        # if needed, print wrong answer\n",
        "        if need_print:\n",
        "            for inp, out, target in zip(prompts, output_batch, targets):\n",
        "                if out != target:\n",
        "                    print(f\"   Input: {inp}\")\n",
        "                    print(f\"  Output: {out}\")\n",
        "                    print(f\"Expected: {target}\")\n",
        "                    print(\"-----------\")\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Accuracy for {num_digits} digits: {acc}\")\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_avg_performance(model, num_digits):\n",
        "    '''\n",
        "    Call this function for get the accuracy for each model\n",
        "    '''\n",
        "    dict_acc = {}\n",
        "    for num_dig in range(1, num_digits+1):\n",
        "        dict_acc[num_dig] = accuracy_print_one(model, num_dig, need_print=False)\n",
        "    return dict_acc\n",
        "\n",
        "def test_accuracy_on_digits(model, digits):\n",
        "    acc_list = []\n",
        "    for i in range(10):\n",
        "        acc_list.append(accuracy_print_one(model, digits, need_print=False))\n",
        "    return sum(acc_list)/len(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij56xIdDLqt1"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)\n",
        "      torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "d1IuDEk1G2Jf",
        "outputId": "4d968f07-f3cd-4379-ef79-49cea74dc8e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfangyua\u001b[0m (\u001b[33mfangyua-univeristy-of-michigan\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "uruNl09gHAQY",
        "outputId": "63dfb48d-2d52-48b7-f7be-b7109189f5da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_022547-mzhv1ot8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8' target=\"_blank\">si for 10</a></strong> to <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7a8ae6176f50>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"transformer_si_graphs\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 35,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            \"max_iter\": 10000\n",
        "            },\n",
        "           name= \"si for 10\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXOIHZeuGIrk",
        "outputId": "f32b3c3a-1f78-4deb-b514-492cc81ae362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File /content/drive/MyDrive/URPS/Data/origin_ds_copy.txt already exists.\n",
            "Skipping generation.\n"
          ]
        }
      ],
      "source": [
        "generate_origin_dataset(original=10, task='copy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoouuP04LAZO"
      },
      "outputs": [],
      "source": [
        "# set_seeds(seed=22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-b60pjqlaw-",
        "outputId": "1f49e275-5900-43e5-da6c-250805ca84e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start run pretrain train loop with 5000 steps and 500 warm, 1000 decay\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   0%|          | 2/5000 [00:05<3:22:01,  2.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: loss 2.6270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   2%|▏         | 101/5000 [00:21<1:22:04,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 100: loss 1.4541\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   4%|▍         | 201/5000 [00:38<2:16:02,  1.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 200: loss 1.1124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   6%|▌         | 301/5000 [00:54<2:19:17,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 300: loss 1.0497\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   8%|▊         | 401/5000 [01:10<2:17:39,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 400: loss 1.0402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  10%|█         | 502/5000 [01:26<1:32:55,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 500: loss 1.0206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  12%|█▏        | 601/5000 [01:42<2:13:39,  1.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 600: loss 1.1558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  14%|█▍        | 701/5000 [01:58<2:08:03,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 700: loss 1.0025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  16%|█▌        | 801/5000 [02:14<2:04:47,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 800: loss 0.9967\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  18%|█▊        | 901/5000 [02:31<2:07:12,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 900: loss 1.0048\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  20%|██        | 1001/5000 [02:47<2:00:00,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1000: loss 1.0072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  22%|██▏       | 1101/5000 [03:03<1:56:55,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1100: loss 0.9842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  24%|██▍       | 1201/5000 [03:20<1:34:15,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1200: loss 1.0022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  26%|██▌       | 1301/5000 [03:36<1:51:46,  1.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1300: loss 0.9831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  28%|██▊       | 1402/5000 [03:52<1:16:38,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1400: loss 0.9849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  30%|███       | 1501/5000 [04:08<1:46:12,  1.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1500: loss 0.9822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  32%|███▏      | 1601/5000 [04:25<1:44:50,  1.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1600: loss 0.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  34%|███▍      | 1701/5000 [04:41<1:38:52,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1700: loss 0.9888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  36%|███▌      | 1801/5000 [04:57<1:31:35,  1.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1800: loss 0.9822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  38%|███▊      | 1901/5000 [05:13<1:17:58,  1.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1900: loss 0.9803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  40%|████      | 2001/5000 [05:29<1:22:08,  1.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2000: loss 0.9816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  42%|████▏     | 2101/5000 [05:46<1:23:39,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2100: loss 0.9837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  44%|████▍     | 2201/5000 [06:02<1:19:31,  1.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2200: loss 0.9792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  46%|████▌     | 2301/5000 [06:18<1:16:46,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2300: loss 0.9793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  48%|████▊     | 2401/5000 [06:34<1:17:29,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2400: loss 0.9801\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  50%|█████     | 2501/5000 [06:50<1:16:13,  1.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2500: loss 0.9797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  52%|█████▏    | 2601/5000 [07:06<59:20,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2600: loss 0.9795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  54%|█████▍    | 2701/5000 [07:22<1:05:24,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2700: loss 0.9785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  56%|█████▌    | 2801/5000 [07:38<46:48,  1.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2800: loss 0.9796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  58%|█████▊    | 2901/5000 [07:54<50:07,  1.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2900: loss 0.9987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  60%|██████    | 3001/5000 [08:10<57:59,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3000: loss 0.9774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  62%|██████▏   | 3101/5000 [08:26<36:50,  1.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3100: loss 0.9782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  64%|██████▍   | 3201/5000 [08:42<50:29,  1.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3200: loss 0.9774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  66%|██████▌   | 3301/5000 [08:58<29:42,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3300: loss 0.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  68%|██████▊   | 3401/5000 [09:14<45:30,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3400: loss 0.9767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  70%|███████   | 3502/5000 [09:30<28:28,  1.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3500: loss 0.9780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  72%|███████▏  | 3601/5000 [09:46<34:56,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3600: loss 0.9780\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  74%|███████▍  | 3701/5000 [10:02<38:52,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3700: loss 0.9799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  76%|███████▌  | 3801/5000 [10:18<35:21,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3800: loss 0.9778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  78%|███████▊  | 3901/5000 [10:34<31:15,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3900: loss 0.9773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  80%|████████  | 4001/5000 [10:50<20:28,  1.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4000: loss 0.9774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  82%|████████▏ | 4101/5000 [11:06<25:45,  1.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4100: loss 0.9776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  84%|████████▍ | 4201/5000 [11:22<23:23,  1.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4200: loss 0.9758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  86%|████████▌ | 4301/5000 [11:39<18:19,  1.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4300: loss 0.9767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  88%|████████▊ | 4401/5000 [11:55<17:59,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4400: loss 0.9755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  90%|█████████ | 4501/5000 [12:11<13:37,  1.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4500: loss 0.9755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  92%|█████████▏| 4601/5000 [12:27<10:51,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4600: loss 0.9753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  94%|█████████▍| 4701/5000 [12:43<06:42,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4700: loss 0.9744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  96%|█████████▌| 4801/5000 [12:59<04:35,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4800: loss 0.9743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  98%|█████████▊| 4901/5000 [13:15<02:51,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4900: loss 0.9746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 5000/5000 [13:31<00:00,  6.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4999: loss 0.9749\n",
            "Training finished for pretrain.\n",
            "Evaluating 11-digit accuracy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Average accuracy: 0.9997\n",
            "Saved best model at /content/drive/MyDrive/URPS/Models/sc_model_0.pt\n"
          ]
        }
      ],
      "source": [
        "# This is a base training loop for producing base model\n",
        "print(f\"Start run pretrain train loop with 5000 steps and 500 warm, 1000 decay\")\n",
        "data = []\n",
        "# INITIALIZE MODEL, OPTIMIZER, SHCEDULER\n",
        "model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias=bias)\n",
        "m = model.to(device)\n",
        "with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.readlines()\n",
        "optimizer, scheduler = create_optimizer_and_scheduler(model, 5000, 500, 1000)\n",
        "\n",
        "# TRAINNG LOOP:\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "loss_list = []\n",
        "\n",
        "scaler = GradScaler('cuda')\n",
        "for iter in tqdm(range(5000), desc=\"Training Progress\"):\n",
        "    # sample a batch of data\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses1 = estimate_loss(data, model)['loss']\n",
        "        print(f\"step {iter}: loss {losses1:.4f}\")\n",
        "        log_dict = {\"Loss\": losses1}\n",
        "        loss_list.append(round(losses1.item(), 4))\n",
        "        wandb.log(log_dict)\n",
        "\n",
        "    xb, yb = get_batch(data)\n",
        "\n",
        "    # evaluate the loss\n",
        "    with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "        logits1, loss1 = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    scaler.scale(loss1).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"Training finished for pretrain.\\nEvaluating 11-digit accuracy...\")\n",
        "\n",
        "# evaluate final performance on digit addition\n",
        "acc = test_accuracy_on_digits(model, 11)\n",
        "print(f\"Average accuracy: {acc}\")\n",
        "filename = f\"sc_model_{0}.pt\"\n",
        "save_path = f\"/content/drive/MyDrive/URPS/Models/{filename}\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Saved best model at {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTQCgwRNLuO7"
      },
      "outputs": [],
      "source": [
        "set_seeds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "FBDTLrFbXSLg",
        "outputId": "b9c8e654-d49d-4d71-f643-b641cf28258e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_215722-jjbh9336</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336' target=\"_blank\">si for 10 rounds with length filter</a></strong> to <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7b7acd9c1550>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "wandb.init(project=\"transformer_si_graphs\",\n",
        "           config={\n",
        "            \"learning_rate\": 5e-4,\n",
        "            \"batch_size\": 1024,\n",
        "            \"block_size\": 35,\n",
        "            \"optimizer\": \"AdamW\",\n",
        "            \"n_embd\": 384,\n",
        "            \"n_head\": 6,\n",
        "            \"n_layer\": 6,\n",
        "            \"dropout\": 0.0,\n",
        "            \"si_iter\": 1500,\n",
        "            \"decay\": 500\n",
        "            },\n",
        "           name= \"si for 10 rounds with length filter\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "collapsed": true,
        "id": "hjyfNVr7cWsd",
        "outputId": "2941534d-a450-42fe-995a-2a10cf0d698b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>█▃▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss</td><td>0.97418</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">si for 10</strong> at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/mzhv1ot8</a><br> View project at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_022547-mzhv1ot8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWloBUp6EAmh"
      },
      "outputs": [],
      "source": [
        "def string_majority_vote_filter(list_of_strings, vote_threshold=0.4):\n",
        "    \"\"\"\n",
        "    Given a list of strings (e.g. predictions from multiple models for ONE prompt),\n",
        "    find the most frequent string. If the top string has >= ceil(threshold * N) votes,\n",
        "    return that string. Otherwise, return None.\n",
        "    \"\"\"\n",
        "    if not list_of_strings:\n",
        "        return None\n",
        "    freq = {}\n",
        "    for s in list_of_strings:\n",
        "        freq[s] = freq.get(s, 0) + 1\n",
        "\n",
        "    best_str, best_count = None, 0\n",
        "    for text, count in freq.items():\n",
        "        if count > best_count:\n",
        "            best_str = text\n",
        "            best_count = count\n",
        "\n",
        "    needed_votes = math.ceil(vote_threshold * len(list_of_strings))\n",
        "    if best_count >= needed_votes:\n",
        "        return best_str\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def gen_si_data_mv(\n",
        "    models,\n",
        "    si_round,\n",
        "    task,\n",
        "    num_samples=300000,\n",
        "    batch_size=1024,\n",
        "    vote_threshold=0.4,  # Lower threshold for harder rounds\n",
        "    max_lines_to_write=50000\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate self-improvement data using majority voting plus length filtering.\n",
        "    This version generates num_samples outputs in batches, and after each batch,\n",
        "    it checks how many valid outputs (i.e. those that pass majority voting and the length filter)\n",
        "    have been written to file. If the target of max_lines_to_write is reached, generation stops.\n",
        "    \"\"\"\n",
        "    output_path = f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_round-1}.txt\"\n",
        "    num_batches = (num_samples) // batch_size + 1\n",
        "    print(f\"Generating SI data for round {si_round} with majority voting...\")\n",
        "\n",
        "    # Clear previous file to prevent accumulation\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "\n",
        "    for batch in range(num_batches):\n",
        "        # Check current number of lines in the output file\n",
        "        if os.path.exists(output_path):\n",
        "            with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                current_lines = sum(1 for _ in f)\n",
        "        else:\n",
        "            current_lines = 0\n",
        "\n",
        "        # If we already have max_lines_to_write, stop early.\n",
        "        if current_lines >= max_lines_to_write:\n",
        "            print(f\"Already reached {max_lines_to_write} lines. Stopping early.\")\n",
        "            break\n",
        "\n",
        "        # 1. Generate a batch of prompts.\n",
        "        prompts = [generate_prompt_OOD(si_round, task, original=10) for _ in range(batch_size)]\n",
        "\n",
        "        # 2. Get predictions from all models.\n",
        "        all_model_outputs = []\n",
        "        for model in models:\n",
        "            encoded = [encode(p) for p in prompts]\n",
        "            prompt_tensor = torch.tensor(encoded, dtype=torch.long, device=device)\n",
        "            outputs = generate(model=model, idx=prompt_tensor, max_new_tokens=35, top_k=1)\n",
        "            all_model_outputs.append(outputs)\n",
        "\n",
        "        # 3. Process each prompt: apply majority voting and then length filtering.\n",
        "        valid_outputs = []\n",
        "        # The length filter checks that the slice starting at index (si_round+11) has exactly (si_round+10) characters.\n",
        "        for i in range(len(prompts)):\n",
        "            # Gather predictions for the i-th prompt.\n",
        "            predictions = [all_model_outputs[m_idx][i] for m_idx in range(len(models))]\n",
        "            best_pred = string_majority_vote_filter(predictions, vote_threshold=vote_threshold)\n",
        "            if best_pred: #  and len(best_pred[(si_round+11):]) == (si_round+10) # NO length filtering now\n",
        "                valid_outputs.append(best_pred)\n",
        "\n",
        "        # 4. Write valid outputs to file, ensuring we do not exceed the target.\n",
        "        remaining = max_lines_to_write - current_lines\n",
        "        to_write = valid_outputs[:remaining]\n",
        "\n",
        "        if to_write:\n",
        "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.writelines([line + \"&\\n\" for line in to_write])\n",
        "\n",
        "        print(f\"Batch {batch+1}/{num_batches}: {current_lines + len(to_write)}/{max_lines_to_write} lines written.\")\n",
        "\n",
        "    # Final check: count the total number of lines written.\n",
        "    if os.path.exists(output_path):\n",
        "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            final_lines = sum(1 for _ in f)\n",
        "    else:\n",
        "        final_lines = 0\n",
        "\n",
        "    print(f\"Writing complete. Total lines written: {final_lines}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In round 1, load from base checkpoints; in later rounds, load the updated models.\n",
        "for si_r in range(1, 11):\n",
        "    # --- Update the list of pretrained models continuously ---\n",
        "    updated_models = []\n",
        "    for i in range(5):\n",
        "        m = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias).to(device)\n",
        "        # For round 1, load base models; for later rounds, load updated ones.\n",
        "        if si_r == 1:\n",
        "            ckpt = f\"/content/drive/MyDrive/URPS/Models/sc_model_0.pt\"\n",
        "        else:\n",
        "            ckpt = f\"/content/drive/MyDrive/URPS/Models/pretrained_model_{i}_round_{si_r-1}.pt\"\n",
        "        m.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "        updated_models.append(m)\n",
        "    models_pretrained = updated_models  # Now these are the continuously updated models\n",
        "\n",
        "    # --- Load the main model from the previous round for training ---\n",
        "    main_model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias).to(device)\n",
        "    main_ckpt = f\"/content/drive/MyDrive/URPS/Models/sc_model_{si_r-1}.pt\"\n",
        "    main_model.load_state_dict(torch.load(main_ckpt, map_location=device))\n",
        "\n",
        "    # --- Generate new SI data using majority voting with updated models ---\n",
        "    gen_si_data_mv(\n",
        "        models=models_pretrained,\n",
        "        si_round=si_r,\n",
        "        task='copy',\n",
        "        num_samples=300000,\n",
        "        batch_size=1024,\n",
        "        vote_threshold=0.4,\n",
        "        max_lines_to_write=50000\n",
        "    )\n",
        "\n",
        "    # --- Get combined data for training ---\n",
        "    data = []\n",
        "    if si_r == 1:\n",
        "        with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data = f.readlines()\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            sub_data = f.readlines()\n",
        "            wrong = 0\n",
        "            for i in range(len(sub_data)):\n",
        "                if sub_data[i][:(si_r+10)] != sub_data[i][(si_r+10+1):(si_r+10+1+si_r+10)]:\n",
        "                    wrong += 1\n",
        "            print(f\"This filtered file has {(wrong / len(sub_data))*100}% wrong answer.\")\n",
        "            data += sub_data * (39+si_r)\n",
        "    else:\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/{si_r-1}_round_combined_ds.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data = f.readlines()\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            sub_data = f.readlines()\n",
        "            wrong = 0\n",
        "            for i in range(len(sub_data)):\n",
        "                if sub_data[i][:(si_r+10)] != sub_data[i][(si_r+10+1):(si_r+10+1+si_r+10)]:\n",
        "                    wrong += 1\n",
        "            print(f\"This filtered file has {(wrong / len(sub_data))*100}% wrong answer.\")\n",
        "            data += sub_data * (39+si_r)\n",
        "    random.shuffle(data)\n",
        "    print(f\"This is round {si_r}, The data used for training has {len(data)/1e6} M rows\")\n",
        "\n",
        "    # --- Training the main model ---\n",
        "    optimizer, scheduler = create_optimizer_and_scheduler(main_model, wandb.config[\"si_iter\"], 0, wandb.config[\"decay\"])\n",
        "    main_model.to(device)\n",
        "    print(sum(p.numel() for p in main_model.parameters())/1e6, 'M parameters')\n",
        "    loss_list = []\n",
        "    scaler = GradScaler('cuda')\n",
        "    train_step = 0\n",
        "\n",
        "    for iter in tqdm(range(wandb.config[\"si_iter\"]), desc=\"Training Progress\"):\n",
        "        if iter % eval_interval == 0 or iter == wandb.config[\"si_iter\"] - 1:\n",
        "            losses = estimate_loss(data, main_model)['loss']\n",
        "            print(f\"step {iter}: loss {losses:.4f}\")\n",
        "            loss_list.append(round(losses.item(), 4))\n",
        "            wandb.log({\"train_loss\": losses.item(), \"train_step\": train_step})\n",
        "            train_step += 1\n",
        "\n",
        "        xb, yb = get_batch(data)\n",
        "\n",
        "        with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "            logits1, loss1 = main_model(xb, yb)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss1).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"Training finished for self-improve round {si_r}.\\nEvaluating {10+si_r+1}-digit accuracy...\")\n",
        "    acc = test_accuracy_on_digits(main_model, 10+si_r+1)\n",
        "    digit_step = 10+si_r+1\n",
        "    wandb.log({\"Accuracy\": acc, \"digit_step\": digit_step})\n",
        "    print(f\"Average accuracy for {10+si_r+1}: {acc}\")\n",
        "    main_save_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{si_r}.pt\"\n",
        "    torch.save(main_model.state_dict(), main_save_path)\n",
        "    print(f\"Saved best main model at {main_save_path}\")\n",
        "\n",
        "    # --- CHANGE B: Update each of the k pretrained models with the new main model's state ---\n",
        "    # This ensures that for the next round, the majority voting models are continuously trained.\n",
        "    for i in range(5):\n",
        "        pretrained_save_path = f\"/content/drive/MyDrive/URPS/Models/pretrained_model_{i}_round_{si_r}.pt\"\n",
        "        # Here we simply copy the main model's state. Alternatively, you could train them independently.\n",
        "        torch.save(main_model.state_dict(), pretrained_save_path)\n",
        "        print(f\"Saved updated pretrained model {i} at {pretrained_save_path}\")\n",
        "\n",
        "    # --- Combine data for future rounds ---\n",
        "    data_smaller, data_larger = [], []\n",
        "    if si_r == 1:\n",
        "        with open(\"/content/drive/MyDrive/URPS/Data/origin_ds_copy.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_larger = f.readlines()\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_smaller = f.readlines()\n",
        "    else:\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/{si_r-1}_round_combined_ds.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_larger = f.readlines()\n",
        "        with open(f\"/content/drive/MyDrive/URPS/Data/si_data_r{si_r-1}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            data_smaller = f.readlines()\n",
        "    print(f\"This is round {si_r}, data larger has {len(data_larger)} rows\")\n",
        "    print(f\"This is round {si_r}, data smaller has {len(data_smaller)} rows\")\n",
        "\n",
        "    data_new = data_larger + data_smaller\n",
        "    random.shuffle(data_new)\n",
        "\n",
        "    combined_save_path = f\"/content/drive/MyDrive/URPS/Data/{si_r}_round_combined_ds.txt\"\n",
        "    with open(combined_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines([line if line.endswith(\"\\n\") else line + \"\\n\" for line in data_new])\n",
        "    print(f\"{si_r}_round_combined_ds.txt has {len(data_new)} rows\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x2mmOIGFLPVB",
        "outputId": "6baa2277-5b42-49b6-a29a-2c62f69cfcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating SI data for round 1 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.034% wrong answer.\n",
            "This is round 1, The data used for training has 4.0 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:31:40,  6.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.2251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 102/1500 [00:23<32:21,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<38:56,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<36:46,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 402/1500 [01:11<21:47,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<29:48,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<26:53,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<24:48,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<21:10,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<15:04,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:50<14:57,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:06<11:45,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:23<09:20,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:39<06:04,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:55<03:00,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:12<00:00,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0014\n",
            "Training finished for self-improve round 1.\n",
            "Evaluating 12-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Average accuracy for 12: 0.9997\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_1.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_1.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_1.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_1.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_1.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_1.pt\n",
            "This is round 1, data larger has 2000000 rows\n",
            "This is round 1, data smaller has 50000 rows\n",
            "1_round_combined_ds.txt has 2050000 rows\n",
            "Generating SI data for round 2 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.058% wrong answer.\n",
            "This is round 2, The data used for training has 4.1 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:31:57,  6.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<44:55,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:18,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<38:47,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0324\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:43,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 502/1500 [01:28<21:46,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<27:01,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:00<24:05,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:17<17:39,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<16:16,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:49<15:18,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:06<13:07,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:22<08:39,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:38<05:43,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:55<02:14,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:11<00:00,  5.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0122\n",
            "Training finished for self-improve round 2.\n",
            "Evaluating 13-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Average accuracy for 13: 0.9991999999999999\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_2.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_2.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_2.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_2.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_2.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_2.pt\n",
            "This is round 2, data larger has 2050000 rows\n",
            "This is round 2, data smaller has 50000 rows\n",
            "2_round_combined_ds.txt has 2100000 rows\n",
            "Generating SI data for round 3 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.074% wrong answer.\n",
            "This is round 3, The data used for training has 4.2 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:32:05,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:49,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:38<39:13,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<35:37,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:11<31:42,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<32:20,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:44<20:44,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:00<24:22,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:16<16:04,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:33<19:21,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:49<14:53,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:06<10:53,  1.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0247\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:22<09:44,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:39<06:06,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:55<02:58,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:12<00:00,  5.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0257\n",
            "Training finished for self-improve round 3.\n",
            "Evaluating 14-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.991\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 14 digits: 0.992\n",
            "Accuracy for 14 digits: 0.991\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.993\n",
            "Average accuracy for 14: 0.9939\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_3.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_3.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_3.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_3.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_3.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_3.pt\n",
            "This is round 3, data larger has 2100000 rows\n",
            "This is round 3, data smaller has 50000 rows\n",
            "3_round_combined_ds.txt has 2150000 rows\n",
            "Generating SI data for round 4 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.466% wrong answer.\n",
            "This is round 4, The data used for training has 4.3 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:45:20,  6.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 102/1500 [00:23<31:03,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<39:52,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 302/1500 [00:56<25:19,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:59,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 502/1500 [01:29<21:39,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<21:13,  1.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<24:24,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<18:52,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<19:29,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<15:07,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<12:05,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<09:07,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:40<06:03,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<02:59,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:14<00:00,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0303\n",
            "Training finished for self-improve round 4.\n",
            "Evaluating 15-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 15 digits: 0.995\n",
            "Accuracy for 15 digits: 0.994\n",
            "Accuracy for 15 digits: 0.994\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 0.996\n",
            "Accuracy for 15 digits: 0.993\n",
            "Accuracy for 15 digits: 0.996\n",
            "Accuracy for 15 digits: 0.996\n",
            "Accuracy for 15 digits: 0.999\n",
            "Average accuracy for 15: 0.9958000000000002\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_4.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_4.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_4.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_4.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_4.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_4.pt\n",
            "This is round 4, data larger has 2150000 rows\n",
            "This is round 4, data smaller has 50000 rows\n",
            "4_round_combined_ds.txt has 2200000 rows\n",
            "Generating SI data for round 5 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.248% wrong answer.\n",
            "This is round 5, The data used for training has 4.4 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:45:09,  6.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 102/1500 [00:22<27:57,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<35:43,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 302/1500 [00:55<26:02,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<32:39,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:09,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<29:21,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:01<24:09,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 802/1500 [02:18<14:40,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:34<18:05,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<10:50,  1.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<11:53,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:23<07:19,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:40<06:05,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:56<03:01,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:13<00:00,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0369\n",
            "Training finished for self-improve round 5.\n",
            "Evaluating 16-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 16 digits: 0.995\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.999\n",
            "Average accuracy for 16: 0.9974999999999999\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_5.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_5.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_5.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_5.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_5.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_5.pt\n",
            "This is round 5, data larger has 2200000 rows\n",
            "This is round 5, data smaller has 50000 rows\n",
            "5_round_combined_ds.txt has 2250000 rows\n",
            "Generating SI data for round 6 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.268% wrong answer.\n",
            "This is round 6, The data used for training has 4.5 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:36:16,  6.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<42:51,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<42:29,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:56<36:42,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<30:39,  1.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:29<30:31,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<26:46,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 702/1500 [02:02<17:08,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 802/1500 [02:18<13:24,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.1020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:34<18:17,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<15:25,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:07<11:58,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<09:49,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<04:45,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<03:01,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:13<00:00,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0435\n",
            "Training finished for self-improve round 6.\n",
            "Evaluating 17-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.994\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.994\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.994\n",
            "Average accuracy for 17: 0.9966999999999999\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_6.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_6.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_6.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_6.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_6.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_6.pt\n",
            "This is round 6, data larger has 2250000 rows\n",
            "This is round 6, data smaller has 50000 rows\n",
            "6_round_combined_ds.txt has 2300000 rows\n",
            "Generating SI data for round 7 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.328% wrong answer.\n",
            "This is round 7, The data used for training has 4.6 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:31:36,  6.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:23<45:55,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<40:19,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:56<36:53,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:21,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:29<29:01,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<27:55,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<23:21,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<15:52,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<18:16,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1002/1500 [02:51<09:57,  1.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<12:29,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:25<09:07,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<06:00,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:58<02:48,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:14<00:00,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0497\n",
            "Training finished for self-improve round 7.\n",
            "Evaluating 18-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.995\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.993\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.996\n",
            "Average accuracy for 18: 0.9964000000000001\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_7.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_7.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_7.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_7.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_7.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_7.pt\n",
            "This is round 7, data larger has 2300000 rows\n",
            "This is round 7, data smaller has 50000 rows\n",
            "7_round_combined_ds.txt has 2350000 rows\n",
            "Generating SI data for round 8 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.306% wrong answer.\n",
            "This is round 8, The data used for training has 4.7 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:34:06,  6.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<40:19,  1.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<43:02,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:56<36:38,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:41,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:28<30:33,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.0620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<27:28,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.1086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<24:30,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 802/1500 [02:18<15:17,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.1034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:34<18:15,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:51<15:18,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<13:13,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.1108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:24<09:11,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<06:05,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:57<03:02,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:13<00:00,  5.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0562\n",
            "Training finished for self-improve round 8.\n",
            "Evaluating 19-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 19 digits: 0.996\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.994\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.996\n",
            "Average accuracy for 19: 0.9974999999999999\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_8.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_8.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_8.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_8.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_8.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_8.pt\n",
            "This is round 8, data larger has 2350000 rows\n",
            "This is round 8, data smaller has 50000 rows\n",
            "8_round_combined_ds.txt has 2400000 rows\n",
            "Generating SI data for round 9 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.308% wrong answer.\n",
            "This is round 9, The data used for training has 4.8 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:35:20,  6.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:22<40:44,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.1073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 201/1500 [00:39<40:04,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 301/1500 [00:55<28:08,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.1108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 401/1500 [01:12<33:40,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.0656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 502/1500 [01:29<21:58,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.1025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:45<27:26,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<21:27,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.1128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:18<20:13,  1.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<18:41,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:52<15:24,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<11:41,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.0637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:25<09:48,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<06:05,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:58<03:04,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:14<00:00,  5.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0611\n",
            "Training finished for self-improve round 9.\n",
            "Evaluating 20-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 20 digits: 1.0\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.996\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.998\n",
            "Accuracy for 20 digits: 0.991\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.994\n",
            "Accuracy for 20 digits: 0.992\n",
            "Average accuracy for 20: 0.9951000000000001\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_9.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_9.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_9.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_9.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_9.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_9.pt\n",
            "This is round 9, data larger has 2400000 rows\n",
            "This is round 9, data smaller has 50000 rows\n",
            "9_round_combined_ds.txt has 2450000 rows\n",
            "Generating SI data for round 10 with majority voting...\n",
            "Batch 1/293: 1024/50000 lines written.\n",
            "Batch 2/293: 2048/50000 lines written.\n",
            "Batch 3/293: 3072/50000 lines written.\n",
            "Batch 4/293: 4096/50000 lines written.\n",
            "Batch 5/293: 5120/50000 lines written.\n",
            "Batch 6/293: 6144/50000 lines written.\n",
            "Batch 7/293: 7168/50000 lines written.\n",
            "Batch 8/293: 8192/50000 lines written.\n",
            "Batch 9/293: 9216/50000 lines written.\n",
            "Batch 10/293: 10240/50000 lines written.\n",
            "Batch 11/293: 11264/50000 lines written.\n",
            "Batch 12/293: 12288/50000 lines written.\n",
            "Batch 13/293: 13312/50000 lines written.\n",
            "Batch 14/293: 14336/50000 lines written.\n",
            "Batch 15/293: 15360/50000 lines written.\n",
            "Batch 16/293: 16384/50000 lines written.\n",
            "Batch 17/293: 17408/50000 lines written.\n",
            "Batch 18/293: 18432/50000 lines written.\n",
            "Batch 19/293: 19456/50000 lines written.\n",
            "Batch 20/293: 20480/50000 lines written.\n",
            "Batch 21/293: 21504/50000 lines written.\n",
            "Batch 22/293: 22528/50000 lines written.\n",
            "Batch 23/293: 23552/50000 lines written.\n",
            "Batch 24/293: 24576/50000 lines written.\n",
            "Batch 25/293: 25600/50000 lines written.\n",
            "Batch 26/293: 26624/50000 lines written.\n",
            "Batch 27/293: 27648/50000 lines written.\n",
            "Batch 28/293: 28672/50000 lines written.\n",
            "Batch 29/293: 29696/50000 lines written.\n",
            "Batch 30/293: 30720/50000 lines written.\n",
            "Batch 31/293: 31744/50000 lines written.\n",
            "Batch 32/293: 32768/50000 lines written.\n",
            "Batch 33/293: 33792/50000 lines written.\n",
            "Batch 34/293: 34816/50000 lines written.\n",
            "Batch 35/293: 35840/50000 lines written.\n",
            "Batch 36/293: 36864/50000 lines written.\n",
            "Batch 37/293: 37888/50000 lines written.\n",
            "Batch 38/293: 38912/50000 lines written.\n",
            "Batch 39/293: 39936/50000 lines written.\n",
            "Batch 40/293: 40960/50000 lines written.\n",
            "Batch 41/293: 41984/50000 lines written.\n",
            "Batch 42/293: 43008/50000 lines written.\n",
            "Batch 43/293: 44032/50000 lines written.\n",
            "Batch 44/293: 45056/50000 lines written.\n",
            "Batch 45/293: 46080/50000 lines written.\n",
            "Batch 46/293: 47104/50000 lines written.\n",
            "Batch 47/293: 48128/50000 lines written.\n",
            "Batch 48/293: 49152/50000 lines written.\n",
            "Batch 49/293: 50000/50000 lines written.\n",
            "Already reached 50000 lines. Stopping early.\n",
            "Writing complete. Total lines written: 50000\n",
            "This filtered file has 0.5680000000000001% wrong answer.\n",
            "This is round 10, The data used for training has 4.9 M rows\n",
            "10.646016 M parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   0%|          | 1/1500 [00:06<2:50:30,  6.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss 1.1706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   7%|▋         | 101/1500 [00:23<42:52,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 100: loss 1.0704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  13%|█▎        | 202/1500 [00:39<28:24,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 200: loss 1.0710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  20%|██        | 302/1500 [00:56<26:21,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 300: loss 1.0667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  27%|██▋       | 402/1500 [01:12<23:58,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 400: loss 1.1031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  33%|███▎      | 501/1500 [01:29<30:56,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500: loss 1.1152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  40%|████      | 601/1500 [01:46<27:51,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 600: loss 1.0692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  47%|████▋     | 701/1500 [02:02<24:35,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 700: loss 1.0682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  53%|█████▎    | 801/1500 [02:19<22:21,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 800: loss 1.0883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  60%|██████    | 901/1500 [02:35<18:26,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 900: loss 1.0685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  67%|██████▋   | 1001/1500 [02:52<15:19,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1000: loss 1.0955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  73%|███████▎  | 1101/1500 [03:08<12:16,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1100: loss 1.1171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  80%|████████  | 1201/1500 [03:25<09:08,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1200: loss 1.0917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  87%|████████▋ | 1301/1500 [03:41<05:52,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1300: loss 1.0688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:  93%|█████████▎| 1401/1500 [03:58<03:02,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1400: loss 1.0674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1500/1500 [04:15<00:00,  5.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 1499: loss 1.0671\n",
            "Training finished for self-improve round 10.\n",
            "Evaluating 21-digit accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 21 digits: 0.996\n",
            "Accuracy for 21 digits: 0.992\n",
            "Accuracy for 21 digits: 0.998\n",
            "Accuracy for 21 digits: 0.994\n",
            "Accuracy for 21 digits: 0.993\n",
            "Accuracy for 21 digits: 0.996\n",
            "Accuracy for 21 digits: 0.991\n",
            "Accuracy for 21 digits: 0.994\n",
            "Accuracy for 21 digits: 0.993\n",
            "Accuracy for 21 digits: 0.996\n",
            "Average accuracy for 21: 0.9943\n",
            "Saved best main model at /content/drive/MyDrive/URPS/Models/sc_model_10.pt\n",
            "Saved updated pretrained model 0 at /content/drive/MyDrive/URPS/Models/pretrained_model_0_round_10.pt\n",
            "Saved updated pretrained model 1 at /content/drive/MyDrive/URPS/Models/pretrained_model_1_round_10.pt\n",
            "Saved updated pretrained model 2 at /content/drive/MyDrive/URPS/Models/pretrained_model_2_round_10.pt\n",
            "Saved updated pretrained model 3 at /content/drive/MyDrive/URPS/Models/pretrained_model_3_round_10.pt\n",
            "Saved updated pretrained model 4 at /content/drive/MyDrive/URPS/Models/pretrained_model_4_round_10.pt\n",
            "This is round 10, data larger has 2450000 rows\n",
            "This is round 10, data smaller has 50000 rows\n",
            "10_round_combined_ds.txt has 2500000 rows\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>█▇▁▃▅▄▄▅▂▁</td></tr><tr><td>digit_step</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>▃▁▂▁▁█▃▄▁▁▂▂▂▃▃▃▂▂▃▃▃▃▃▇▃▃▃▃▃▅▄▅▃▅▄▃█▄▅▄</td></tr><tr><td>train_step</td><td>▁▃▃▄▃▅▅▆▇▁▆▁▃▇▇▃▄▅▅█▁▃▃▅▇▅▇█▃▇▁▂▄▅▆▁▃▃▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.9943</td></tr><tr><td>digit_step</td><td>21</td></tr><tr><td>train_loss</td><td>1.06709</td></tr><tr><td>train_step</td><td>15</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">si for 10 rounds with length filter</strong> at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/jjbh9336</a><br> View project at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_215722-jjbh9336/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDfWl2Yoj2eg",
        "outputId": "0d3aa6b1-24a7-4992-cb52-f1ab5908f0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 0.991\n",
            "Accuracy for 12 digits: 0.988\n",
            "Accuracy for 12 digits: 0.99\n",
            "Accuracy for 12 digits: 0.991\n",
            "Accuracy for 12 digits: 0.981\n",
            "Accuracy for 12 digits: 0.984\n",
            "Accuracy for 12 digits: 0.989\n",
            "Accuracy for 12 digits: 0.984\n",
            "Accuracy for 12 digits: 0.987\n",
            "Accuracy for 12 digits: 0.984\n",
            "Accuracy for 13 digits: 0.809\n",
            "Accuracy for 13 digits: 0.814\n",
            "Accuracy for 13 digits: 0.81\n",
            "Accuracy for 13 digits: 0.809\n",
            "Accuracy for 13 digits: 0.802\n",
            "Accuracy for 13 digits: 0.831\n",
            "Accuracy for 13 digits: 0.848\n",
            "Accuracy for 13 digits: 0.806\n",
            "Accuracy for 13 digits: 0.791\n",
            "Accuracy for 13 digits: 0.835\n",
            "Accuracy for 14 digits: 0.363\n",
            "Accuracy for 14 digits: 0.388\n",
            "Accuracy for 14 digits: 0.392\n",
            "Accuracy for 14 digits: 0.375\n",
            "Accuracy for 14 digits: 0.36\n",
            "Accuracy for 14 digits: 0.351\n",
            "Accuracy for 14 digits: 0.373\n",
            "Accuracy for 14 digits: 0.387\n",
            "Accuracy for 14 digits: 0.376\n",
            "Accuracy for 14 digits: 0.376\n",
            "Accuracy for 15 digits: 0.058\n",
            "Accuracy for 15 digits: 0.049\n",
            "Accuracy for 15 digits: 0.05\n",
            "Accuracy for 15 digits: 0.052\n",
            "Accuracy for 15 digits: 0.06\n",
            "Accuracy for 15 digits: 0.051\n",
            "Accuracy for 15 digits: 0.044\n",
            "Accuracy for 15 digits: 0.072\n",
            "Accuracy for 15 digits: 0.046\n",
            "Accuracy for 15 digits: 0.057\n",
            "Accuracy for 16 digits: 0.002\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.002\n",
            "Accuracy for 16 digits: 0.002\n",
            "Accuracy for 16 digits: 0.001\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 16 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 17 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 18 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.997\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 0.996\n",
            "Accuracy for 13 digits: 0.995\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.996\n",
            "Accuracy for 13 digits: 0.996\n",
            "Accuracy for 13 digits: 0.994\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.996\n",
            "Accuracy for 14 digits: 0.969\n",
            "Accuracy for 14 digits: 0.984\n",
            "Accuracy for 14 digits: 0.97\n",
            "Accuracy for 14 digits: 0.967\n",
            "Accuracy for 14 digits: 0.963\n",
            "Accuracy for 14 digits: 0.976\n",
            "Accuracy for 14 digits: 0.97\n",
            "Accuracy for 14 digits: 0.961\n",
            "Accuracy for 14 digits: 0.975\n",
            "Accuracy for 14 digits: 0.971\n",
            "Accuracy for 15 digits: 0.871\n",
            "Accuracy for 15 digits: 0.859\n",
            "Accuracy for 15 digits: 0.848\n",
            "Accuracy for 15 digits: 0.865\n",
            "Accuracy for 15 digits: 0.856\n",
            "Accuracy for 15 digits: 0.856\n",
            "Accuracy for 15 digits: 0.869\n",
            "Accuracy for 15 digits: 0.86\n",
            "Accuracy for 15 digits: 0.88\n",
            "Accuracy for 15 digits: 0.859\n",
            "Accuracy for 16 digits: 0.55\n",
            "Accuracy for 16 digits: 0.563\n",
            "Accuracy for 16 digits: 0.556\n",
            "Accuracy for 16 digits: 0.587\n",
            "Accuracy for 16 digits: 0.587\n",
            "Accuracy for 16 digits: 0.552\n",
            "Accuracy for 16 digits: 0.563\n",
            "Accuracy for 16 digits: 0.569\n",
            "Accuracy for 16 digits: 0.563\n",
            "Accuracy for 16 digits: 0.55\n",
            "Accuracy for 17 digits: 0.153\n",
            "Accuracy for 17 digits: 0.16\n",
            "Accuracy for 17 digits: 0.175\n",
            "Accuracy for 17 digits: 0.16\n",
            "Accuracy for 17 digits: 0.139\n",
            "Accuracy for 17 digits: 0.166\n",
            "Accuracy for 17 digits: 0.17\n",
            "Accuracy for 17 digits: 0.162\n",
            "Accuracy for 17 digits: 0.173\n",
            "Accuracy for 17 digits: 0.159\n",
            "Accuracy for 18 digits: 0.013\n",
            "Accuracy for 18 digits: 0.01\n",
            "Accuracy for 18 digits: 0.008\n",
            "Accuracy for 18 digits: 0.009\n",
            "Accuracy for 18 digits: 0.006\n",
            "Accuracy for 18 digits: 0.007\n",
            "Accuracy for 18 digits: 0.016\n",
            "Accuracy for 18 digits: 0.01\n",
            "Accuracy for 18 digits: 0.009\n",
            "Accuracy for 18 digits: 0.007\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.993\n",
            "Accuracy for 14 digits: 0.989\n",
            "Accuracy for 14 digits: 0.99\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.99\n",
            "Accuracy for 14 digits: 0.993\n",
            "Accuracy for 14 digits: 0.992\n",
            "Accuracy for 14 digits: 0.993\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 15 digits: 0.914\n",
            "Accuracy for 15 digits: 0.932\n",
            "Accuracy for 15 digits: 0.928\n",
            "Accuracy for 15 digits: 0.903\n",
            "Accuracy for 15 digits: 0.929\n",
            "Accuracy for 15 digits: 0.931\n",
            "Accuracy for 15 digits: 0.908\n",
            "Accuracy for 15 digits: 0.918\n",
            "Accuracy for 15 digits: 0.917\n",
            "Accuracy for 15 digits: 0.912\n",
            "Accuracy for 16 digits: 0.641\n",
            "Accuracy for 16 digits: 0.606\n",
            "Accuracy for 16 digits: 0.637\n",
            "Accuracy for 16 digits: 0.612\n",
            "Accuracy for 16 digits: 0.624\n",
            "Accuracy for 16 digits: 0.645\n",
            "Accuracy for 16 digits: 0.654\n",
            "Accuracy for 16 digits: 0.629\n",
            "Accuracy for 16 digits: 0.639\n",
            "Accuracy for 16 digits: 0.636\n",
            "Accuracy for 17 digits: 0.217\n",
            "Accuracy for 17 digits: 0.211\n",
            "Accuracy for 17 digits: 0.241\n",
            "Accuracy for 17 digits: 0.23\n",
            "Accuracy for 17 digits: 0.215\n",
            "Accuracy for 17 digits: 0.227\n",
            "Accuracy for 17 digits: 0.231\n",
            "Accuracy for 17 digits: 0.212\n",
            "Accuracy for 17 digits: 0.242\n",
            "Accuracy for 17 digits: 0.22\n",
            "Accuracy for 18 digits: 0.029\n",
            "Accuracy for 18 digits: 0.028\n",
            "Accuracy for 18 digits: 0.028\n",
            "Accuracy for 18 digits: 0.031\n",
            "Accuracy for 18 digits: 0.027\n",
            "Accuracy for 18 digits: 0.026\n",
            "Accuracy for 18 digits: 0.023\n",
            "Accuracy for 18 digits: 0.031\n",
            "Accuracy for 18 digits: 0.029\n",
            "Accuracy for 18 digits: 0.032\n",
            "Accuracy for 19 digits: 0.004\n",
            "Accuracy for 19 digits: 0.002\n",
            "Accuracy for 19 digits: 0.001\n",
            "Accuracy for 19 digits: 0.001\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.003\n",
            "Accuracy for 19 digits: 0.001\n",
            "Accuracy for 19 digits: 0.001\n",
            "Accuracy for 19 digits: 0.0\n",
            "Accuracy for 19 digits: 0.001\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 20 digits: 0.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.997\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.997\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.996\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.996\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.997\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 14 digits: 0.994\n",
            "Accuracy for 14 digits: 0.991\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.992\n",
            "Accuracy for 14 digits: 0.995\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.992\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.993\n",
            "Accuracy for 15 digits: 0.987\n",
            "Accuracy for 15 digits: 0.989\n",
            "Accuracy for 15 digits: 0.986\n",
            "Accuracy for 15 digits: 0.992\n",
            "Accuracy for 15 digits: 0.98\n",
            "Accuracy for 15 digits: 0.986\n",
            "Accuracy for 15 digits: 0.992\n",
            "Accuracy for 15 digits: 0.99\n",
            "Accuracy for 15 digits: 0.991\n",
            "Accuracy for 15 digits: 0.983\n",
            "Accuracy for 16 digits: 0.966\n",
            "Accuracy for 16 digits: 0.959\n",
            "Accuracy for 16 digits: 0.97\n",
            "Accuracy for 16 digits: 0.971\n",
            "Accuracy for 16 digits: 0.975\n",
            "Accuracy for 16 digits: 0.971\n",
            "Accuracy for 16 digits: 0.971\n",
            "Accuracy for 16 digits: 0.978\n",
            "Accuracy for 16 digits: 0.969\n",
            "Accuracy for 16 digits: 0.963\n",
            "Accuracy for 17 digits: 0.92\n",
            "Accuracy for 17 digits: 0.924\n",
            "Accuracy for 17 digits: 0.917\n",
            "Accuracy for 17 digits: 0.939\n",
            "Accuracy for 17 digits: 0.922\n",
            "Accuracy for 17 digits: 0.938\n",
            "Accuracy for 17 digits: 0.926\n",
            "Accuracy for 17 digits: 0.915\n",
            "Accuracy for 17 digits: 0.936\n",
            "Accuracy for 17 digits: 0.937\n",
            "Accuracy for 18 digits: 0.855\n",
            "Accuracy for 18 digits: 0.872\n",
            "Accuracy for 18 digits: 0.856\n",
            "Accuracy for 18 digits: 0.832\n",
            "Accuracy for 18 digits: 0.855\n",
            "Accuracy for 18 digits: 0.869\n",
            "Accuracy for 18 digits: 0.862\n",
            "Accuracy for 18 digits: 0.862\n",
            "Accuracy for 18 digits: 0.848\n",
            "Accuracy for 18 digits: 0.845\n",
            "Accuracy for 19 digits: 0.749\n",
            "Accuracy for 19 digits: 0.78\n",
            "Accuracy for 19 digits: 0.749\n",
            "Accuracy for 19 digits: 0.742\n",
            "Accuracy for 19 digits: 0.761\n",
            "Accuracy for 19 digits: 0.773\n",
            "Accuracy for 19 digits: 0.768\n",
            "Accuracy for 19 digits: 0.744\n",
            "Accuracy for 19 digits: 0.773\n",
            "Accuracy for 19 digits: 0.752\n",
            "Accuracy for 20 digits: 0.609\n",
            "Accuracy for 20 digits: 0.617\n",
            "Accuracy for 20 digits: 0.634\n",
            "Accuracy for 20 digits: 0.605\n",
            "Accuracy for 20 digits: 0.633\n",
            "Accuracy for 20 digits: 0.646\n",
            "Accuracy for 20 digits: 0.647\n",
            "Accuracy for 20 digits: 0.624\n",
            "Accuracy for 20 digits: 0.618\n",
            "Accuracy for 20 digits: 0.642\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.997\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 0.992\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.993\n",
            "Accuracy for 15 digits: 0.996\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.99\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 16 digits: 0.992\n",
            "Accuracy for 16 digits: 0.989\n",
            "Accuracy for 16 digits: 0.994\n",
            "Accuracy for 16 digits: 0.994\n",
            "Accuracy for 16 digits: 0.988\n",
            "Accuracy for 16 digits: 0.992\n",
            "Accuracy for 16 digits: 0.987\n",
            "Accuracy for 16 digits: 0.989\n",
            "Accuracy for 16 digits: 0.991\n",
            "Accuracy for 16 digits: 0.991\n",
            "Accuracy for 17 digits: 0.969\n",
            "Accuracy for 17 digits: 0.972\n",
            "Accuracy for 17 digits: 0.966\n",
            "Accuracy for 17 digits: 0.963\n",
            "Accuracy for 17 digits: 0.964\n",
            "Accuracy for 17 digits: 0.965\n",
            "Accuracy for 17 digits: 0.97\n",
            "Accuracy for 17 digits: 0.969\n",
            "Accuracy for 17 digits: 0.971\n",
            "Accuracy for 17 digits: 0.969\n",
            "Accuracy for 18 digits: 0.931\n",
            "Accuracy for 18 digits: 0.893\n",
            "Accuracy for 18 digits: 0.914\n",
            "Accuracy for 18 digits: 0.911\n",
            "Accuracy for 18 digits: 0.923\n",
            "Accuracy for 18 digits: 0.895\n",
            "Accuracy for 18 digits: 0.915\n",
            "Accuracy for 18 digits: 0.902\n",
            "Accuracy for 18 digits: 0.91\n",
            "Accuracy for 18 digits: 0.896\n",
            "Accuracy for 19 digits: 0.77\n",
            "Accuracy for 19 digits: 0.73\n",
            "Accuracy for 19 digits: 0.779\n",
            "Accuracy for 19 digits: 0.749\n",
            "Accuracy for 19 digits: 0.783\n",
            "Accuracy for 19 digits: 0.749\n",
            "Accuracy for 19 digits: 0.75\n",
            "Accuracy for 19 digits: 0.775\n",
            "Accuracy for 19 digits: 0.767\n",
            "Accuracy for 19 digits: 0.757\n",
            "Accuracy for 20 digits: 0.519\n",
            "Accuracy for 20 digits: 0.511\n",
            "Accuracy for 20 digits: 0.518\n",
            "Accuracy for 20 digits: 0.51\n",
            "Accuracy for 20 digits: 0.517\n",
            "Accuracy for 20 digits: 0.522\n",
            "Accuracy for 20 digits: 0.529\n",
            "Accuracy for 20 digits: 0.51\n",
            "Accuracy for 20 digits: 0.515\n",
            "Accuracy for 20 digits: 0.495\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.996\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.995\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.997\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.995\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 17 digits: 0.979\n",
            "Accuracy for 17 digits: 0.968\n",
            "Accuracy for 17 digits: 0.98\n",
            "Accuracy for 17 digits: 0.982\n",
            "Accuracy for 17 digits: 0.977\n",
            "Accuracy for 17 digits: 0.98\n",
            "Accuracy for 17 digits: 0.985\n",
            "Accuracy for 17 digits: 0.983\n",
            "Accuracy for 17 digits: 0.974\n",
            "Accuracy for 17 digits: 0.98\n",
            "Accuracy for 18 digits: 0.888\n",
            "Accuracy for 18 digits: 0.898\n",
            "Accuracy for 18 digits: 0.909\n",
            "Accuracy for 18 digits: 0.923\n",
            "Accuracy for 18 digits: 0.921\n",
            "Accuracy for 18 digits: 0.917\n",
            "Accuracy for 18 digits: 0.909\n",
            "Accuracy for 18 digits: 0.908\n",
            "Accuracy for 18 digits: 0.893\n",
            "Accuracy for 18 digits: 0.913\n",
            "Accuracy for 19 digits: 0.712\n",
            "Accuracy for 19 digits: 0.715\n",
            "Accuracy for 19 digits: 0.734\n",
            "Accuracy for 19 digits: 0.734\n",
            "Accuracy for 19 digits: 0.762\n",
            "Accuracy for 19 digits: 0.757\n",
            "Accuracy for 19 digits: 0.746\n",
            "Accuracy for 19 digits: 0.717\n",
            "Accuracy for 19 digits: 0.712\n",
            "Accuracy for 19 digits: 0.744\n",
            "Accuracy for 20 digits: 0.434\n",
            "Accuracy for 20 digits: 0.484\n",
            "Accuracy for 20 digits: 0.459\n",
            "Accuracy for 20 digits: 0.465\n",
            "Accuracy for 20 digits: 0.476\n",
            "Accuracy for 20 digits: 0.478\n",
            "Accuracy for 20 digits: 0.459\n",
            "Accuracy for 20 digits: 0.44\n",
            "Accuracy for 20 digits: 0.456\n",
            "Accuracy for 20 digits: 0.447\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 0.995\n",
            "Accuracy for 17 digits: 0.994\n",
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.995\n",
            "Accuracy for 17 digits: 0.995\n",
            "Accuracy for 18 digits: 0.976\n",
            "Accuracy for 18 digits: 0.977\n",
            "Accuracy for 18 digits: 0.982\n",
            "Accuracy for 18 digits: 0.971\n",
            "Accuracy for 18 digits: 0.985\n",
            "Accuracy for 18 digits: 0.983\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.978\n",
            "Accuracy for 18 digits: 0.985\n",
            "Accuracy for 18 digits: 0.981\n",
            "Accuracy for 19 digits: 0.845\n",
            "Accuracy for 19 digits: 0.833\n",
            "Accuracy for 19 digits: 0.816\n",
            "Accuracy for 19 digits: 0.833\n",
            "Accuracy for 19 digits: 0.836\n",
            "Accuracy for 19 digits: 0.833\n",
            "Accuracy for 19 digits: 0.828\n",
            "Accuracy for 19 digits: 0.829\n",
            "Accuracy for 19 digits: 0.831\n",
            "Accuracy for 19 digits: 0.841\n",
            "Accuracy for 20 digits: 0.413\n",
            "Accuracy for 20 digits: 0.445\n",
            "Accuracy for 20 digits: 0.435\n",
            "Accuracy for 20 digits: 0.447\n",
            "Accuracy for 20 digits: 0.403\n",
            "Accuracy for 20 digits: 0.419\n",
            "Accuracy for 20 digits: 0.406\n",
            "Accuracy for 20 digits: 0.411\n",
            "Accuracy for 20 digits: 0.443\n",
            "Accuracy for 20 digits: 0.388\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.998\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.996\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 18 digits: 0.99\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 0.995\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.995\n",
            "Accuracy for 19 digits: 0.989\n",
            "Accuracy for 19 digits: 0.99\n",
            "Accuracy for 19 digits: 0.992\n",
            "Accuracy for 19 digits: 0.99\n",
            "Accuracy for 19 digits: 0.988\n",
            "Accuracy for 19 digits: 0.991\n",
            "Accuracy for 19 digits: 0.989\n",
            "Accuracy for 19 digits: 0.99\n",
            "Accuracy for 19 digits: 0.993\n",
            "Accuracy for 19 digits: 0.994\n",
            "Accuracy for 20 digits: 0.961\n",
            "Accuracy for 20 digits: 0.958\n",
            "Accuracy for 20 digits: 0.952\n",
            "Accuracy for 20 digits: 0.957\n",
            "Accuracy for 20 digits: 0.945\n",
            "Accuracy for 20 digits: 0.958\n",
            "Accuracy for 20 digits: 0.957\n",
            "Accuracy for 20 digits: 0.955\n",
            "Accuracy for 20 digits: 0.948\n",
            "Accuracy for 20 digits: 0.973\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.997\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 19 digits: 0.996\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 0.992\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.994\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 20 digits: 0.99\n",
            "Accuracy for 20 digits: 0.99\n",
            "Accuracy for 20 digits: 0.993\n",
            "Accuracy for 20 digits: 0.993\n",
            "Accuracy for 20 digits: 0.994\n",
            "Accuracy for 20 digits: 0.99\n",
            "Accuracy for 20 digits: 0.993\n",
            "Accuracy for 20 digits: 0.992\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.991\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 0.999\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.998\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.996\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.997\n",
            "Accuracy for 20 digits: 0.994\n",
            "Accuracy for 20 digits: 0.992\n",
            "Accuracy for 20 digits: 0.993\n",
            "Accuracy for 20 digits: 0.994\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.992\n",
            "Accuracy for 20 digits: 0.993\n",
            "Accuracy for 20 digits: 0.992\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 11 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 0.999\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 12 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.998\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 13 digits: 0.999\n",
            "Accuracy for 13 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.998\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 14 digits: 0.999\n",
            "Accuracy for 14 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.998\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 1.0\n",
            "Accuracy for 15 digits: 0.999\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.998\n",
            "Accuracy for 16 digits: 1.0\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 16 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 17 digits: 0.997\n",
            "Accuracy for 17 digits: 0.996\n",
            "Accuracy for 17 digits: 1.0\n",
            "Accuracy for 17 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 1.0\n",
            "Accuracy for 18 digits: 0.997\n",
            "Accuracy for 18 digits: 0.998\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 18 digits: 0.999\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 1.0\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.999\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 19 digits: 0.996\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.998\n",
            "Accuracy for 19 digits: 0.996\n",
            "Accuracy for 19 digits: 0.995\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.995\n",
            "Accuracy for 20 digits: 0.994\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.997\n",
            "Accuracy for 20 digits: 0.998\n",
            "Accuracy for 20 digits: 0.998\n",
            "Accuracy for 20 digits: 0.996\n"
          ]
        }
      ],
      "source": [
        "diff_model_performance = {}\n",
        "for i in range (11):\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias)\n",
        "    model.to(device)\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{i}.pt\"\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "    one_list = []\n",
        "    for j in range(11, 21):\n",
        "        acc = test_accuracy_on_digits(model, j)\n",
        "        one_list.append(acc)\n",
        "    diff_model_performance[i] = one_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "7Ihomw5i40Tg",
        "outputId": "43ebaedb-b7b3-466f-9fbb-14433104fc3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5e6b4b06-0831-49a6-a149-83811e7a8c48\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5e6b4b06-0831-49a6-a149-83811e7a8c48\")) {                    Plotly.newPlot(                        \"5e6b4b06-0831-49a6-a149-83811e7a8c48\",                        [{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 0\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9994,0.9869,0.8155000000000001,0.3741,0.0539,0.0007,0.0,0.0,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 1\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[1.0,0.9994,0.9964000000000001,0.9705999999999999,0.8623,0.564,0.1617,0.0095,0.0,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 2\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9998999999999999,0.9998000000000001,0.9987999999999999,0.9926,0.9192000000000002,0.6323000000000001,0.22460000000000005,0.0284,0.0014000000000000002,0.0],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 3\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9991999999999999,0.9982,0.9983000000000001,0.9943000000000002,0.9875999999999999,0.9692999999999998,0.9273999999999999,0.8555999999999999,0.7590999999999999,0.6275000000000001],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 4\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9994,0.9997000000000001,0.9992000000000001,0.9978,0.9961,0.9906999999999998,0.9677999999999999,0.9090000000000001,0.7609,0.5146],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 5\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9999,0.9996,0.9996,0.9988999999999999,0.9991,0.9968,0.9788,0.9079,0.7333,0.4598000000000001],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 6\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9997999999999999,0.9997999999999999,0.9997,0.9994,0.9996,0.9983999999999998,0.9963,0.9808,0.8324999999999999,0.421],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 7\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9998999999999999,0.9996,0.9994999999999999,1.0,0.9996,0.9987999999999999,0.9984,0.9959,0.9905999999999999,0.9564],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 8\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[1.0,0.9997,0.9994999999999999,0.9993000000000001,0.9992999999999999,0.9995,0.9985000000000002,0.9982000000000001,0.9962,0.9921],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 9\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[0.9997999999999999,0.9999,1.0,1.0,0.9999,0.9996,0.9994999999999999,0.9987,0.9984,0.9936999999999999],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Self-improvement round 10\",\"x\":[11,12,13,14,15,16,17,18,19,20],\"y\":[1.0,0.9998000000000001,0.9995,0.9991999999999999,0.9996,0.999,0.9987,0.9991000000000001,0.9974000000000001,0.9965999999999999],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"10 rounds of self-improvement, with majority voting\"},\"xaxis\":{\"title\":{\"text\":\"number of digits\"},\"tickmode\":\"array\",\"tickvals\":[11,12,13,14,15,16,17,18,19,20]},\"yaxis\":{\"title\":{\"text\":\"Average Accuracy\"},\"range\":[-0.02,1.02]},\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5e6b4b06-0831-49a6-a149-83811e7a8c48');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250320_233120-dui1nm4i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/dui1nm4i' target=\"_blank\">si for 10 rounds with majority voting</a></strong> to <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/dui1nm4i' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/dui1nm4i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">si for 10 rounds with majority voting</strong> at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/dui1nm4i' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs/runs/dui1nm4i</a><br> View project at: <a href='https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs' target=\"_blank\">https://wandb.ai/fangyua-univeristy-of-michigan/transformer_si_graphs</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250320_233120-dui1nm4i/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "x_values = [i for i in range(11, 21)]\n",
        "\n",
        "\n",
        "i = 0\n",
        "for m_performace in diff_model_performance.values():\n",
        "    fig.add_trace(go.Scatter(x=x_values,\n",
        "                             y=m_performace,\n",
        "                             mode='lines+markers',\n",
        "\n",
        "                             name=f\"Self-improvement round {i}\"))\n",
        "    i += 1\n",
        "\n",
        "fig.update_layout(title=\"10 rounds of self-improvement, with majority voting\", xaxis_title=\"number of digits\", yaxis_title=\"Average Accuracy\")\n",
        "fig.update_layout(xaxis_title=\"number of digits\", yaxis_title=\"Average Accuracy\")\n",
        "fig.update_yaxes(range=[-0.02, 1.02])\n",
        "fig.update_xaxes(tickmode=\"array\", tickvals=x_values)\n",
        "fig.update_layout(width=1000, height=500)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "wandb.init(project=\"transformer_si_graphs\", name=\"si for 10 rounds with majority voting\")\n",
        "wandb.log({\"Interactive Chart\": wandb.Html(fig.to_html())})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g36D8p-J4JyB"
      },
      "outputs": [],
      "source": [
        "def save_wrong_answers(si_data_file, si_round):\n",
        "    \"\"\"\n",
        "    Reads the SI data file and saves lines where the expected answer (first si_round+10 characters)\n",
        "    does not match the generated answer (the subsequent si_round+10 characters after an '=' token)\n",
        "    into a wrong answers file.\n",
        "    \"\"\"\n",
        "    wrong_answers = []\n",
        "    with open(si_data_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "    for line in lines:\n",
        "        # Assuming the expected answer is in the first (si_round+10) characters and\n",
        "        # the generated answer is in the substring starting at index (si_round+10+1)\n",
        "        expected = line[:(si_round+10)]\n",
        "        generated = line[(si_round+10+1):(si_round+10+1+si_round+10)]\n",
        "        if expected != generated:\n",
        "            wrong_answers.append(line)\n",
        "    wrong_filename = f\"/content/drive/MyDrive/URPS/Data/wrong_answers_round_{si_round}.txt\"\n",
        "    with open(wrong_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(wrong_answers)\n",
        "    print(f\"Round {si_round}: Saved {len(wrong_answers)} wrong answers to {wrong_filename}\")\n",
        "    return wrong_filename\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate each saved model (sc_model_0 through sc_model_10)\n",
        "for r in range(0, 11):\n",
        "    # -----------------------------------\n",
        "    # Step 1: Load the model checkpoint.\n",
        "    # -----------------------------------\n",
        "    model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias).to(device)\n",
        "    ckpt_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{r}.pt\"\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "    print(f\"Loaded model checkpoint: sc_model_{r}.pt\")\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Step 2: Determine evaluation parameters.\n",
        "    # -----------------------------------\n",
        "    # For example, if in round 0 (the base model) we evaluate on 11-digit task,\n",
        "    # then for round r, you might evaluate on (10 + r + 1)-digit tasks.\n",
        "    # Here we set current_si_round to 1 for r==0 and r for r>=1.\n",
        "    current_si_round = r if r > 0 else 1\n",
        "    digit_step = 10 + current_si_round + 1  # e.g., 11 for round 0, 12 for round 1, etc.\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Step 3: Generate a batch of SI data using the current model.\n",
        "    # -----------------------------------\n",
        "    # Generate a sample of 1000 prompts. The prompt length is determined by (original + current_si_round).\n",
        "    sample_prompts = [generate_prompt_OOD(current_si_round, 'copy', original=10) for _ in range(50000)]\n",
        "    encoded_prompts = [encode(p) for p in sample_prompts]\n",
        "    prompt_tensor = torch.tensor(encoded_prompts, dtype=torch.long, device=device)\n",
        "    # Generate outputs using the current model.\n",
        "    batch_size_generate = 1024  # or smaller, adjust as needed\n",
        "    outputs = []\n",
        "    for i in range(0, prompt_tensor.shape[0], batch_size_generate):\n",
        "        batch_outputs = generate(model=model, idx=prompt_tensor[i:i + batch_size_generate], max_new_tokens=35, top_k=1)\n",
        "        outputs.extend(batch_outputs)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Step 4: Save the SI data to a temporary file.\n",
        "    # -----------------------------------\n",
        "    temp_si_data_file = f\"/content/drive/MyDrive/URPS/Data/temp_si_data_round_{r}.txt\"\n",
        "    with open(temp_si_data_file, \"w\", encoding=\"utf-8\") as f:\n",
        "         f.writelines([line + \"&\\n\" for line in outputs])\n",
        "    print(f\"Saved temporary SI data for round {r} to {temp_si_data_file}\")\n",
        "\n",
        "    # -----------------------------------\n",
        "    # Step 5: Extract and save wrong answers.\n",
        "    # -----------------------------------\n",
        "    # The helper function `save_wrong_answers` will compare the expected portion (first current_si_round+10 characters)\n",
        "    # with the generated portion (the next current_si_round+10 characters after an offset) and save the wrong ones.\n",
        "    wrong_file = save_wrong_answers(temp_si_data_file, current_si_round)\n",
        "    # wrong_file will be something like \"wrong_answers_round_{current_si_round}.txt\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELkrhQchOR7e",
        "outputId": "8a277be2-22ed-4fd6-877e-b99a91df17be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model checkpoint: sc_model_0.pt\n",
            "Saved temporary SI data for round 0 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_0.txt\n",
            "Round 1: Saved 16 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_1.txt\n",
            "Loaded model checkpoint: sc_model_1.pt\n",
            "Saved temporary SI data for round 1 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_1.txt\n",
            "Round 1: Saved 8 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_1.txt\n",
            "Loaded model checkpoint: sc_model_2.pt\n",
            "Saved temporary SI data for round 2 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_2.txt\n",
            "Round 2: Saved 8 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_2.txt\n",
            "Loaded model checkpoint: sc_model_3.pt\n",
            "Saved temporary SI data for round 3 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_3.txt\n",
            "Round 3: Saved 99 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_3.txt\n",
            "Loaded model checkpoint: sc_model_4.pt\n",
            "Saved temporary SI data for round 4 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_4.txt\n",
            "Round 4: Saved 67 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_4.txt\n",
            "Loaded model checkpoint: sc_model_5.pt\n",
            "Saved temporary SI data for round 5 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_5.txt\n",
            "Round 5: Saved 40 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_5.txt\n",
            "Loaded model checkpoint: sc_model_6.pt\n",
            "Saved temporary SI data for round 6 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_6.txt\n",
            "Round 6: Saved 43 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_6.txt\n",
            "Loaded model checkpoint: sc_model_7.pt\n",
            "Saved temporary SI data for round 7 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_7.txt\n",
            "Round 7: Saved 80 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_7.txt\n",
            "Loaded model checkpoint: sc_model_8.pt\n",
            "Saved temporary SI data for round 8 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_8.txt\n",
            "Round 8: Saved 74 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_8.txt\n",
            "Loaded model checkpoint: sc_model_9.pt\n",
            "Saved temporary SI data for round 9 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_9.txt\n",
            "Round 9: Saved 82 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_9.txt\n",
            "Loaded model checkpoint: sc_model_10.pt\n",
            "Saved temporary SI data for round 10 to /content/drive/MyDrive/URPS/Data/temp_si_data_round_10.txt\n",
            "Round 10: Saved 175 wrong answers to /content/drive/MyDrive/URPS/Data/wrong_answers_round_10.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_wrong_answers_accuracy(model, wrong_file, si_round):\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance on wrong answer samples.\n",
        "\n",
        "    Each line in wrong_file is assumed to have the format:\n",
        "         <expected>=<generated>&\\n\n",
        "    where <expected> is the correct answer (of length si_round+10) and\n",
        "    <generated> is the model-generated answer (of length si_round+10) following an \"=\".\n",
        "\n",
        "    The function constructs a prompt as <expected> + \"=\",\n",
        "    then generates an output from the model and extracts the generated portion.\n",
        "    It then compares this new generated answer with <expected> and computes\n",
        "    the fraction of samples for which the model now produces the correct expected answer.\n",
        "\n",
        "    Parameters:\n",
        "      model: The GPT model used for generation.\n",
        "      wrong_file: Path to the file containing wrong answer samples.\n",
        "      si_round: The self-improvement round number used to determine expected lengths.\n",
        "\n",
        "    Returns:\n",
        "      accuracy: The fraction of samples that the model now corrects.\n",
        "    \"\"\"\n",
        "    # Read all wrong-answer lines from the file.\n",
        "    with open(wrong_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    total = len(lines)\n",
        "    if total == 0:\n",
        "        print(\"No wrong answer samples found.\")\n",
        "        return 0.0\n",
        "\n",
        "    correct_count = 0\n",
        "    # Loop through each wrong answer sample.\n",
        "    for line in lines:\n",
        "        # Extract the expected answer.\n",
        "        expected = line[:(si_round+10)]\n",
        "        # Construct the prompt for generation.\n",
        "        prompt = expected + \"=\"\n",
        "        # Encode the prompt.\n",
        "        prompt_ids = encode(prompt)\n",
        "        prompt_tensor = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
        "        # Generate a new output using the model.\n",
        "        new_output = generate(model=model, idx=prompt_tensor, max_new_tokens=35, top_k=1)[0]\n",
        "        # Extract the generated part (assumed to be of length si_round+10 immediately after the '=' token).\n",
        "        new_generated = new_output[(si_round+10+1):(si_round+10+1+si_round+10)]\n",
        "        # If the new generated answer matches the expected answer, count it as corrected.\n",
        "        if new_generated == expected:\n",
        "            correct_count += 1\n",
        "\n",
        "    accuracy = correct_count / total\n",
        "    print(f\"Evaluated {total} wrong samples; model corrected {correct_count} of them. Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "6YdAdF_IZMkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each round T (from 1 to 9, for example),\n",
        "# evaluate the wrong_answers_round_T.txt file using the checkpoints from round T+1 to round 10.\n",
        "for t in range(1, 10):  # You can adjust this range as needed.\n",
        "    wrong_eval_file = f\"/content/drive/MyDrive/URPS/Data/wrong_answers_round_{t}.txt\"\n",
        "    print(f\"\\nEvaluating wrong answers from round {t}:\")\n",
        "\n",
        "    # Loop over subsequent checkpoints\n",
        "    for r in range(t+1, 11):\n",
        "        model = GPT(vocab_size, block_size, n_embd, n_layer, n_head, dropout, bias).to(device)\n",
        "        ckpt_path = f\"/content/drive/MyDrive/URPS/Models/sc_model_{r}.pt\"\n",
        "        model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "\n",
        "        # Evaluate the model on the wrong answers from round t.\n",
        "        acc = test_wrong_answers_accuracy(model, wrong_eval_file, t)\n",
        "        print(f\"  Model sc_model_{r} accuracy on wrong answers from round {t}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "MJmwMS-yQUnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f7a7f2-cb36-4414-f21e-675b76b256b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating wrong answers from round 1:\n",
            "Evaluated 8 wrong samples; model corrected 7 of them. Accuracy: 0.8750\n",
            "  Model sc_model_2 accuracy on wrong answers from round 1: 0.8750\n",
            "Evaluated 8 wrong samples; model corrected 4 of them. Accuracy: 0.5000\n",
            "  Model sc_model_3 accuracy on wrong answers from round 1: 0.5000\n",
            "Evaluated 8 wrong samples; model corrected 6 of them. Accuracy: 0.7500\n",
            "  Model sc_model_4 accuracy on wrong answers from round 1: 0.7500\n",
            "Evaluated 8 wrong samples; model corrected 6 of them. Accuracy: 0.7500\n",
            "  Model sc_model_5 accuracy on wrong answers from round 1: 0.7500\n",
            "Evaluated 8 wrong samples; model corrected 7 of them. Accuracy: 0.8750\n",
            "  Model sc_model_6 accuracy on wrong answers from round 1: 0.8750\n",
            "Evaluated 8 wrong samples; model corrected 6 of them. Accuracy: 0.7500\n",
            "  Model sc_model_7 accuracy on wrong answers from round 1: 0.7500\n",
            "Evaluated 8 wrong samples; model corrected 7 of them. Accuracy: 0.8750\n",
            "  Model sc_model_8 accuracy on wrong answers from round 1: 0.8750\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_9 accuracy on wrong answers from round 1: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 7 of them. Accuracy: 0.8750\n",
            "  Model sc_model_10 accuracy on wrong answers from round 1: 0.8750\n",
            "\n",
            "Evaluating wrong answers from round 2:\n",
            "Evaluated 8 wrong samples; model corrected 7 of them. Accuracy: 0.8750\n",
            "  Model sc_model_3 accuracy on wrong answers from round 2: 0.8750\n",
            "Evaluated 8 wrong samples; model corrected 6 of them. Accuracy: 0.7500\n",
            "  Model sc_model_4 accuracy on wrong answers from round 2: 0.7500\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_5 accuracy on wrong answers from round 2: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_6 accuracy on wrong answers from round 2: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_7 accuracy on wrong answers from round 2: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_8 accuracy on wrong answers from round 2: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_9 accuracy on wrong answers from round 2: 1.0000\n",
            "Evaluated 8 wrong samples; model corrected 8 of them. Accuracy: 1.0000\n",
            "  Model sc_model_10 accuracy on wrong answers from round 2: 1.0000\n",
            "\n",
            "Evaluating wrong answers from round 3:\n",
            "Evaluated 99 wrong samples; model corrected 91 of them. Accuracy: 0.9192\n",
            "  Model sc_model_4 accuracy on wrong answers from round 3: 0.9192\n",
            "Evaluated 99 wrong samples; model corrected 96 of them. Accuracy: 0.9697\n",
            "  Model sc_model_5 accuracy on wrong answers from round 3: 0.9697\n",
            "Evaluated 99 wrong samples; model corrected 95 of them. Accuracy: 0.9596\n",
            "  Model sc_model_6 accuracy on wrong answers from round 3: 0.9596\n",
            "Evaluated 99 wrong samples; model corrected 97 of them. Accuracy: 0.9798\n",
            "  Model sc_model_7 accuracy on wrong answers from round 3: 0.9798\n",
            "Evaluated 99 wrong samples; model corrected 98 of them. Accuracy: 0.9899\n",
            "  Model sc_model_8 accuracy on wrong answers from round 3: 0.9899\n",
            "Evaluated 99 wrong samples; model corrected 97 of them. Accuracy: 0.9798\n",
            "  Model sc_model_9 accuracy on wrong answers from round 3: 0.9798\n",
            "Evaluated 99 wrong samples; model corrected 97 of them. Accuracy: 0.9798\n",
            "  Model sc_model_10 accuracy on wrong answers from round 3: 0.9798\n",
            "\n",
            "Evaluating wrong answers from round 4:\n",
            "Evaluated 67 wrong samples; model corrected 54 of them. Accuracy: 0.8060\n",
            "  Model sc_model_5 accuracy on wrong answers from round 4: 0.8060\n",
            "Evaluated 67 wrong samples; model corrected 60 of them. Accuracy: 0.8955\n",
            "  Model sc_model_6 accuracy on wrong answers from round 4: 0.8955\n",
            "Evaluated 67 wrong samples; model corrected 61 of them. Accuracy: 0.9104\n",
            "  Model sc_model_7 accuracy on wrong answers from round 4: 0.9104\n",
            "Evaluated 67 wrong samples; model corrected 65 of them. Accuracy: 0.9701\n",
            "  Model sc_model_8 accuracy on wrong answers from round 4: 0.9701\n",
            "Evaluated 67 wrong samples; model corrected 66 of them. Accuracy: 0.9851\n",
            "  Model sc_model_9 accuracy on wrong answers from round 4: 0.9851\n",
            "Evaluated 67 wrong samples; model corrected 66 of them. Accuracy: 0.9851\n",
            "  Model sc_model_10 accuracy on wrong answers from round 4: 0.9851\n",
            "\n",
            "Evaluating wrong answers from round 5:\n",
            "Evaluated 40 wrong samples; model corrected 35 of them. Accuracy: 0.8750\n",
            "  Model sc_model_6 accuracy on wrong answers from round 5: 0.8750\n",
            "Evaluated 40 wrong samples; model corrected 35 of them. Accuracy: 0.8750\n",
            "  Model sc_model_7 accuracy on wrong answers from round 5: 0.8750\n",
            "Evaluated 40 wrong samples; model corrected 40 of them. Accuracy: 1.0000\n",
            "  Model sc_model_8 accuracy on wrong answers from round 5: 1.0000\n",
            "Evaluated 40 wrong samples; model corrected 39 of them. Accuracy: 0.9750\n",
            "  Model sc_model_9 accuracy on wrong answers from round 5: 0.9750\n",
            "Evaluated 40 wrong samples; model corrected 39 of them. Accuracy: 0.9750\n",
            "  Model sc_model_10 accuracy on wrong answers from round 5: 0.9750\n",
            "\n",
            "Evaluating wrong answers from round 6:\n",
            "Evaluated 43 wrong samples; model corrected 28 of them. Accuracy: 0.6512\n",
            "  Model sc_model_7 accuracy on wrong answers from round 6: 0.6512\n",
            "Evaluated 43 wrong samples; model corrected 33 of them. Accuracy: 0.7674\n",
            "  Model sc_model_8 accuracy on wrong answers from round 6: 0.7674\n",
            "Evaluated 43 wrong samples; model corrected 35 of them. Accuracy: 0.8140\n",
            "  Model sc_model_9 accuracy on wrong answers from round 6: 0.8140\n",
            "Evaluated 43 wrong samples; model corrected 34 of them. Accuracy: 0.7907\n",
            "  Model sc_model_10 accuracy on wrong answers from round 6: 0.7907\n",
            "\n",
            "Evaluating wrong answers from round 7:\n",
            "Evaluated 80 wrong samples; model corrected 54 of them. Accuracy: 0.6750\n",
            "  Model sc_model_8 accuracy on wrong answers from round 7: 0.6750\n",
            "Evaluated 80 wrong samples; model corrected 67 of them. Accuracy: 0.8375\n",
            "  Model sc_model_9 accuracy on wrong answers from round 7: 0.8375\n",
            "Evaluated 80 wrong samples; model corrected 56 of them. Accuracy: 0.7000\n",
            "  Model sc_model_10 accuracy on wrong answers from round 7: 0.7000\n",
            "\n",
            "Evaluating wrong answers from round 8:\n",
            "Evaluated 74 wrong samples; model corrected 45 of them. Accuracy: 0.6081\n",
            "  Model sc_model_9 accuracy on wrong answers from round 8: 0.6081\n",
            "Evaluated 74 wrong samples; model corrected 41 of them. Accuracy: 0.5541\n",
            "  Model sc_model_10 accuracy on wrong answers from round 8: 0.5541\n",
            "\n",
            "Evaluating wrong answers from round 9:\n",
            "Evaluated 82 wrong samples; model corrected 21 of them. Accuracy: 0.2561\n",
            "  Model sc_model_10 accuracy on wrong answers from round 9: 0.2561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Example evaluation data.\n",
        "# For each wrong answer round T, we have:\n",
        "#   - A list of checkpoints (from T+1 to 10)\n",
        "#   - A corresponding list of accuracies (fraction corrected)\n",
        "evaluation_data = {\n",
        "    1: ([2, 3, 4, 5, 6, 7, 8, 9, 10], [0.8750, 0.5000, 0.7500, 0.7500, 0.8750, 0.7500, 0.8750, 1.0000, 0.8750]),\n",
        "    2: ([3, 4, 5, 6, 7, 8, 9, 10], [0.8750, 0.7500, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
        "    3: ([4, 5, 6, 7, 8, 9, 10], [0.9192, 0.9697, 0.9596, 0.9798, 0.9899, 0.9798, 0.9798]),\n",
        "    4: ([5, 6, 7, 8, 9, 10], [0.8060, 0.8955, 0.9104, 0.9701, 0.9851, 0.9851]),\n",
        "    5: ([6, 7, 8, 9, 10], [0.8750, 0.8750, 1.0000, 0.9750, 0.9750]),\n",
        "    6: ([7, 8, 9, 10], [0.6512, 0.7674, 0.8140, 0.7907]),\n",
        "    7: ([8, 9, 10], [0.6750, 0.8375, 0.7000]),\n",
        "    8: ([9, 10], [0.6081, 0.5541]),\n",
        "    9: ([10], [0.2561])\n",
        "}\n",
        "\n",
        "# Create an interactive Plotly graph.\n",
        "fig = go.Figure()\n",
        "\n",
        "for t, (checkpoints, accuracies) in evaluation_data.items():\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=checkpoints,\n",
        "        y=accuracies,\n",
        "        mode='lines+markers',\n",
        "        name=f\"Wrong Answers from Round {t}\",\n",
        "        hovertemplate=\"Checkpoint: %{x}<br>Accuracy: %{y:.4f}<extra></extra>\"\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Model Accuracy on Wrong Answers Across Rounds\",\n",
        "    xaxis_title=\"Model Checkpoint Round\",\n",
        "    yaxis_title=\"Accuracy on Wrong Answers\",\n",
        "    hovermode=\"x unified\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9dCDiXqEb7xp",
        "outputId": "ea637623-0710-40c0-9e03-591e992b9e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"c2665b93-7f3f-45ac-a2dd-472ce70cc0de\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c2665b93-7f3f-45ac-a2dd-472ce70cc0de\")) {                    Plotly.newPlot(                        \"c2665b93-7f3f-45ac-a2dd-472ce70cc0de\",                        [{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 1\",\"x\":[2,3,4,5,6,7,8,9,10],\"y\":[0.875,0.5,0.75,0.75,0.875,0.75,0.875,1.0,0.875],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 2\",\"x\":[3,4,5,6,7,8,9,10],\"y\":[0.875,0.75,1.0,1.0,1.0,1.0,1.0,1.0],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 3\",\"x\":[4,5,6,7,8,9,10],\"y\":[0.9192,0.9697,0.9596,0.9798,0.9899,0.9798,0.9798],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 4\",\"x\":[5,6,7,8,9,10],\"y\":[0.806,0.8955,0.9104,0.9701,0.9851,0.9851],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 5\",\"x\":[6,7,8,9,10],\"y\":[0.875,0.875,1.0,0.975,0.975],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 6\",\"x\":[7,8,9,10],\"y\":[0.6512,0.7674,0.814,0.7907],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 7\",\"x\":[8,9,10],\"y\":[0.675,0.8375,0.7],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 8\",\"x\":[9,10],\"y\":[0.6081,0.5541],\"type\":\"scatter\"},{\"hovertemplate\":\"Checkpoint: %{x}\\u003cbr\\u003eAccuracy: %{y:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"mode\":\"lines+markers\",\"name\":\"Wrong Answers from Round 9\",\"x\":[10],\"y\":[0.2561],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Interactive Accuracy on Wrong Answers Across Rounds\"},\"xaxis\":{\"title\":{\"text\":\"Model Checkpoint Round\"}},\"yaxis\":{\"title\":{\"text\":\"Accuracy on Wrong Answers\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c2665b93-7f3f-45ac-a2dd-472ce70cc0de');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}